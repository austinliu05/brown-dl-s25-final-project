{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9062847,"sourceType":"datasetVersion","datasetId":5465455}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Useful commamnds\n!rm -rf /kaggle/working/*","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:05:49.676698Z","iopub.execute_input":"2025-04-22T04:05:49.678165Z","iopub.status.idle":"2025-04-22T04:05:49.818431Z","shell.execute_reply.started":"2025-04-22T04:05:49.678116Z","shell.execute_reply":"2025-04-22T04:05:49.817210Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pwd\n!ls -a","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:48:25.236749Z","iopub.execute_input":"2025-04-22T04:48:25.237767Z","iopub.status.idle":"2025-04-22T04:48:25.536403Z","shell.execute_reply.started":"2025-04-22T04:48:25.237737Z","shell.execute_reply":"2025-04-22T04:48:25.535198Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working\n.  ..  .virtual_documents\n","output_type":"stream"}],"execution_count":61},{"cell_type":"markdown","source":"Reference Github: https://github.com/BreezeWhite/oemer","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom PIL import Image\nimport xml.etree.ElementTree as ET\nfrom tqdm import tqdm\nimport os\nimport matplotlib.pyplot as plt\n\ndoremi_dir = \"/kaggle/input/doremi-raw-dataset/DoReMi_v1/\"\nIMAGE_WIDTH = 2475\nIMAGE_HEIGHT = 1577","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:05:49.821036Z","iopub.execute_input":"2025-04-22T04:05:49.821346Z","iopub.status.idle":"2025-04-22T04:05:49.847995Z","shell.execute_reply.started":"2025-04-22T04:05:49.821322Z","shell.execute_reply":"2025-04-22T04:05:49.847126Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Sample data\n\nImporting sample image from the [DoReMi](https://github.com/steinbergmedia/DoReMi/?tab=readme-ov-file#OMR-metadata) dataset.","metadata":{}},{"cell_type":"code","source":"img_path = doremi_dir + 'Images/Alkan - Posement-001.png'\nimage = Image.open(img_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:05:49.848970Z","iopub.execute_input":"2025-04-22T04:05:49.849195Z","iopub.status.idle":"2025-04-22T04:05:49.943196Z","shell.execute_reply.started":"2025-04-22T04:05:49.849177Z","shell.execute_reply":"2025-04-22T04:05:49.942405Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:05:49.943982Z","iopub.execute_input":"2025-04-22T04:05:49.944194Z","iopub.status.idle":"2025-04-22T04:05:50.000659Z","shell.execute_reply.started":"2025-04-22T04:05:49.944178Z","shell.execute_reply":"2025-04-22T04:05:49.999712Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"file_path = doremi_dir + 'OMR_XML/Alkan - Posement-layout-0-muscima.xml'\nwith open(file_path, 'r') as file:\n    content = file.read(200)\n    print(content)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:05:50.003615Z","iopub.execute_input":"2025-04-22T04:05:50.003912Z","iopub.status.idle":"2025-04-22T04:05:50.015311Z","shell.execute_reply.started":"2025-04-22T04:05:50.003890Z","shell.execute_reply":"2025-04-22T04:05:50.014367Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Preprocessing Data","metadata":{}},{"cell_type":"markdown","source":"## Cropping Input Image\nTo adjust the size of our data, we wrote a python script to crop the input image. ","metadata":{}},{"cell_type":"code","source":"from IPython.display import display\n\ndef crop_top_half(image_path):\n    \"\"\"\n    Crops the top half of the image.\n\n    Parameters:\n    - image_path: str, path to the input image.\n\n    Returns:\n    - cropped_image: PIL.Image.Image, the cropped top half of the image.\n    \"\"\"\n    image = Image.open(image_path)\n\n    width, height = image.size\n    crop_height = height * 0.45\n    crop_box = (0, 0, width, crop_height)\n    cropped_image = image.crop(crop_box)\n    \n    return cropped_image\n\n# Example usage\ninput_image_path = doremi_dir + 'Images/Alkan - Posement-001.png'\ncropped_image = crop_top_half(input_image_path)\n\ndisplay(cropped_image)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:05:50.016543Z","iopub.execute_input":"2025-04-22T04:05:50.016923Z","iopub.status.idle":"2025-04-22T04:05:50.056636Z","shell.execute_reply.started":"2025-04-22T04:05:50.016893Z","shell.execute_reply":"2025-04-22T04:05:50.055628Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Staffline Extraction\nFirst, we will extract stafflines. This step is the most important as the music scores are determined by their relative position to the stafflines. We will extract the following information for each staffline, which are fundamental for the later parts of this project. ","metadata":{}},{"cell_type":"code","source":"# example output of staff extraction\n# Staff {\n#     Center: 0.0 # y-center \n#     Upper bound: 150 # upper bound of the block\n#     Lower bound: 200 # lower bound of the block\n#     Unit size: 5.0 # distance between each stafflines\n#     Track: 1 # for a two-handed piano score, track 1 would be for the left hand and track 2 would be for right hand\n#     Group: 3 # for a two-handed piano score, two tracks are grouped into one\n# }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:05:50.057901Z","iopub.execute_input":"2025-04-22T04:05:50.058236Z","iopub.status.idle":"2025-04-22T04:05:50.062601Z","shell.execute_reply.started":"2025-04-22T04:05:50.058203Z","shell.execute_reply":"2025-04-22T04:05:50.061662Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Line:\n    def __init__(self) -> None:\n        self.points = [] # stores the (x,y) pixels that make up the line\n        self.label = \"\" # FIRST, SECOND, THIRD, FOURTH, FIFTH\n\n    def add_point(self, x:int, y:int) -> None:\n        self.points.append((x,y))\n        self.y_center = None\n        self.upper = None\n        self.lower = None\n        self.x_center = None\n        self.x_left = None\n        self.x_right = None \n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:05:50.063728Z","iopub.execute_input":"2025-04-22T04:05:50.064448Z","iopub.status.idle":"2025-04-22T04:05:50.082283Z","shell.execute_reply.started":"2025-04-22T04:05:50.064416Z","shell.execute_reply":"2025-04-22T04:05:50.081426Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Staff:\n    def __init(self) -> None:\n        self.lines = List[Line]\n        self.track = None\n        self.group = None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:05:50.083456Z","iopub.execute_input":"2025-04-22T04:05:50.083920Z","iopub.status.idle":"2025-04-22T04:05:50.100871Z","shell.execute_reply.started":"2025-04-22T04:05:50.083889Z","shell.execute_reply":"2025-04-22T04:05:50.099928Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Boundary Thought Process: ","metadata":{}},{"cell_type":"code","source":"class Staff:\n    def __init__(self, center: float, upper_bound: int, lower_bound: int, unit_size: float, track: int):\n        self.center = center\n        self.upper_bound = upper_bound\n        self.lower_bound = lower_bound\n        self.unit_size = unit_size\n        self.track = track\n\n    def __repr__(self):\n        return (\n            \"Staff {\\n\"\n            f\"    Center: {self.center:.1f}\\n\"\n            f\"    Upper bound: {self.upper_bound}\\n\"\n            f\"    Lower bound: {self.lower_bound}\\n\"\n            f\"    Unit size: {self.unit_size:.1f}\\n\"\n            f\"    Track: {self.track}\\n\"\n            \"}\"\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:05:50.102031Z","iopub.execute_input":"2025-04-22T04:05:50.102434Z","iopub.status.idle":"2025-04-22T04:05:50.118247Z","shell.execute_reply.started":"2025-04-22T04:05:50.102407Z","shell.execute_reply":"2025-04-22T04:05:50.117290Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nfrom scipy.signal import find_peaks\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Step 0: Convert grayscale\nimage_gray = cropped_image.convert(\"L\")\nimage_np = np.array(image_gray)\n\n# Step 1: Binarize image\n_, binary = cv2.threshold(image_np, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n\n# Step 2: Horizontal projection\nhorizontal_projection = np.sum(binary, axis=1)\n\n# Step 3: Detect staffline peaks\npeaks, _ = find_peaks(horizontal_projection, height=np.max(horizontal_projection) * 0.5, distance=5)\n\n# Step 4: Group into staves (5 lines per group)\ngrouped_staffs = []\nfor i in range(0, len(peaks) - 4, 5):\n    group = peaks[i:i+5]\n    grouped_staffs.append(group)\n\n# Step 5: Convert grayscale to RGB for drawing\nimage_color = np.stack([image_np]*3, axis=-1)\n\n# Step 6: Draw bounding boxes and print structured output\nstaff_bounds = []\nstaff_bounds = []\nstaff_objects = []\n\nfor i, group in enumerate(grouped_staffs):\n    y_center = float(np.mean(group))\n    unit_size = float(np.mean(np.diff(group)))\n    padding = int(unit_size * 3.5)\n\n    y_min = int(np.min(group) - padding)\n    y_max = int(np.max(group) + padding)\n    track = i % 2 + 1\n\n    staff = Staff(\n        center=y_center,\n        upper_bound=y_min,\n        lower_bound=y_max,\n        unit_size=unit_size,\n        track=track\n    )\n    staff_objects.append(staff)\n    staff_bounds.append((y_min, y_max))\n\n    cv2.rectangle(image_color, (0, y_min), (image_np.shape[1], y_max), (0, 255, 0), 2)\n    print(staff)\n\n\n# Step 7: Show image with bounding boxes\nplt.figure(figsize=(20, 20))\nplt.imshow(image_color)\nplt.axis(\"off\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:05:50.119310Z","iopub.execute_input":"2025-04-22T04:05:50.119620Z","iopub.status.idle":"2025-04-22T04:05:52.434243Z","shell.execute_reply.started":"2025-04-22T04:05:50.119586Z","shell.execute_reply":"2025-04-22T04:05:52.433476Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Extracting First 200 One Staff Images","metadata":{}},{"cell_type":"code","source":"import re\nimport pprint\n\ndef natural_key(string):\n    return [int(s) if s.isdigit() else s.lower() for s in re.split('(\\d+)', string)]\n\n#Sorts Images and Parsed_by_page_omr_xml directories\nimage_list = os.listdir(doremi_dir+ \"Images\")\nomr_xml_list =  os.listdir(doremi_dir + \"Parsed_by_page_omr_xml\")\nimage_list.sort(key=natural_key)\nomr_xml_list.sort(key=natural_key)\n\n#Matches each .png file to their respective omr_xml file\nimage_to_omr = {img: omr for img, omr in zip(image_list, omr_xml_list)}\n#pprint.pprint(image_to_omr)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:05:52.435177Z","iopub.execute_input":"2025-04-22T04:05:52.435661Z","iopub.status.idle":"2025-04-22T04:05:52.932449Z","shell.execute_reply.started":"2025-04-22T04:05:52.435643Z","shell.execute_reply":"2025-04-22T04:05:52.931766Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_staffs_from_image(cropped_image, padding_multiplier=4.5):\n    image_gray = cropped_image.convert(\"L\")\n    image_np = np.array(image_gray)\n    _, binary = cv2.threshold(image_np, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n    horizontal_projection = np.sum(binary, axis=1)\n    peaks, _ = find_peaks(horizontal_projection, height=np.max(horizontal_projection) * 0.5, distance=5)\n\n    grouped_staffs = []\n    for i in range(0, len(peaks) - 4, 5):\n        group = peaks[i:i+5]\n        grouped_staffs.append(group)\n\n    staff_objects = []\n    for i, group in enumerate(grouped_staffs):\n        y_center = float(np.mean(group))\n        unit_size = float(np.mean(np.diff(group)))\n        padding = int(unit_size * padding_multiplier)\n\n        y_min = int(np.min(group) - padding)\n        y_max = int(np.max(group) + padding)\n        track = i % 2 + 1\n\n        staff = Staff(\n            center=y_center,\n            upper_bound=y_min,\n            lower_bound=y_max,\n            unit_size=unit_size,\n            track=track\n        )\n        staff_objects.append(staff)\n\n    return staff_objects","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:05:52.933367Z","iopub.execute_input":"2025-04-22T04:05:52.933683Z","iopub.status.idle":"2025-04-22T04:05:52.941698Z","shell.execute_reply.started":"2025-04-22T04:05:52.933659Z","shell.execute_reply":"2025-04-22T04:05:52.940792Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from PIL import Image\nimport os\n\nsingle_track_images = []\nstaff_data_per_image = {}\n\nfor image_name in image_list:\n    image_path = os.path.join(doremi_dir, \"Images\", image_name)\n    try:\n        cropped_image = Image.open(image_path)\n        staff_objects = extract_staffs_from_image(cropped_image)\n\n        tracks = set(s.track for s in staff_objects)\n        if tracks == {1}:  # Only track 1\n            single_track_images.append(image_name)\n            staff_data_per_image[image_name] = staff_objects  # store staff info\n\n        if len(single_track_images) == 200:\n            break\n    except Exception as e:\n        print(f\"Skipping {image_name} due to error: {e}\")\n        continue\n\n# Confirm results\nprint(f\"\\nFound {len(single_track_images)} single-track images.\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:05:52.944807Z","iopub.execute_input":"2025-04-22T04:05:52.945072Z","iopub.status.idle":"2025-04-22T04:06:20.475795Z","shell.execute_reply.started":"2025-04-22T04:05:52.945054Z","shell.execute_reply":"2025-04-22T04:06:20.474873Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"To help see what those 200 1 staff images we are going to use look like, (ALSO HELPS IF WE WANT TO FILTER OUT ANY INDIVIDUAL DATA OR IMAGES)","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom PIL import Image\nimport os\n\n# First 50 and last 50 images\nfirst_single_200 = single_track_images\n\ncols = 5\nrows = (200 + cols - 1) // cols\n\nplt.figure(figsize=(20, rows * 4))\n\nfor i, img_name in enumerate(first_single_200):\n    image_path = os.path.join(doremi_dir + \"Images\", img_name)\n    img = Image.open(image_path)\n    \n    plt.subplot(rows, cols, i + 1)\n    plt.imshow(img, cmap=\"gray\")\n    plt.title(img_name, fontsize=8)\n    plt.axis(\"off\")\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:06:20.476742Z","iopub.execute_input":"2025-04-22T04:06:20.477089Z","iopub.status.idle":"2025-04-22T04:09:37.556561Z","shell.execute_reply.started":"2025-04-22T04:06:20.477061Z","shell.execute_reply":"2025-04-22T04:09:37.555243Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"max_height = 0\nmax_image_name = None\nmax_upper = None\nmax_lower = None\n\nfor image_name in first_single_200:\n    image_path = os.path.join(doremi_dir + \"Images\", image_name)\n    cropped_image = Image.open(image_path)\n\n    staff_objects = extract_staffs_from_image(cropped_image)\n    #print(staff_objects)\n    if max_upper == None or staff_objects[0].upper_bound < max_upper:\n        max_upper = staff_objects[0].upper_bound\n    if max_lower == None or staff_objects[0].lower_bound > max_lower:\n        max_lower = staff_objects[0].lower_bound\n\nprint(f\"Upper bound: {max_upper}\")\nprint(f\"Lower bound: {max_lower}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:09:37.557628Z","iopub.execute_input":"2025-04-22T04:09:37.557934Z","iopub.status.idle":"2025-04-22T04:09:47.795645Z","shell.execute_reply.started":"2025-04-22T04:09:37.557912Z","shell.execute_reply":"2025-04-22T04:09:47.794639Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# OMR_XML Preprocessng","metadata":{}},{"cell_type":"code","source":"import re\nimport pprint\n\ndef natural_key(string):\n    return [int(s) if s.isdigit() else s.lower() for s in re.split('(\\d+)', string)]\n\n#Sorts Images and Parsed_by_page_omr_xml directories\nimage_list = os.listdir(doremi_dir+ \"Images\")\nomr_xml_list =  os.listdir(doremi_dir + \"Parsed_by_page_omr_xml\")\nimage_list.sort(key=natural_key)\nomr_xml_list.sort(key=natural_key)\n\n#Matches each .png file to their respective omr_xml file\nimage_to_omr = {img: omr for img, omr in zip(image_list, omr_xml_list)}\n#pprint.pprint(image_to_omr)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:09:47.796892Z","iopub.execute_input":"2025-04-22T04:09:47.797238Z","iopub.status.idle":"2025-04-22T04:09:47.868469Z","shell.execute_reply.started":"2025-04-22T04:09:47.797207Z","shell.execute_reply":"2025-04-22T04:09:47.867668Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#To view an image [0,5217]\nkey_list = list(image_to_omr.keys())\nimage_path = os.path.join(doremi_dir, \"Images\", key_list[5217])\nimage = Image.open(image_path)\n#image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:09:47.869241Z","iopub.execute_input":"2025-04-22T04:09:47.869449Z","iopub.status.idle":"2025-04-22T04:09:47.878859Z","shell.execute_reply.started":"2025-04-22T04:09:47.869433Z","shell.execute_reply":"2025-04-22T04:09:47.877993Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Note:\n    def __init__(self, class_name: str, top: int, left: int, width: int, height: int):\n        self.class_name = class_name\n        self.top = top\n        self.left = left\n        self.width = width\n        self.height = height\n    def gather_coordinates(self):\n        return [self.top, self.left, self.width, self.height]\n    def create_box(self):\n        return [(self.left, self.top), (self.left + self.width, self.top + self.height)]\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:09:47.879665Z","iopub.execute_input":"2025-04-22T04:09:47.879931Z","iopub.status.idle":"2025-04-22T04:09:47.894528Z","shell.execute_reply.started":"2025-04-22T04:09:47.879904Z","shell.execute_reply":"2025-04-22T04:09:47.893727Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import xml.etree.ElementTree as ET\n\nimage_to_notes = {}\nfor key in key_list:\n# Parse the XML file\n    xml_path = os.path.join(doremi_dir, \"Parsed_by_page_omr_xml\", image_to_omr[key])\n    tree = ET.parse(xml_path)  # replace with your actual file path\n    root = tree.getroot()\n# Define target classnames\n    target_classes = {'noteheadBlack', 'noteheadHalf', 'noteheadWhole'}\n\n# Collect matching nodes\n    matching_nodes = []\n\n# Loop through all <Node> elements\n    for node in root.findall('.//Node'):\n        class_name = node.find('ClassName')\n        if class_name is not None and class_name.text in ('noteheadBlack', 'noteheadHalf', 'noteheadWhole'):\n            matching_nodes.append(node)\n\n\n# Print results\n    final_notes = []\n    for match in matching_nodes:\n        note = Note(match.find(\"ClassName\").text, int(match.find(\"Top\").text), int(match.find(\"Left\").text), int(match.find(\"Width\").text), int(match.find(\"Height\").text))\n        final_notes.append(note)\n    image_to_notes[key] = final_notes\n#pprint.pprint(image_to_notes)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:09:47.895509Z","iopub.execute_input":"2025-04-22T04:09:47.895732Z","iopub.status.idle":"2025-04-22T04:10:30.715888Z","shell.execute_reply.started":"2025-04-22T04:09:47.895715Z","shell.execute_reply":"2025-04-22T04:10:30.715197Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"first_200_image_to_notes = {key: image_to_notes[key] for key in first_single_200 if key in image_to_notes}\n\ntop = max_upper\nbottom = max_lower\n\ncropped_note_pairs = []\nfor i, img_name in enumerate(first_200_image_to_notes.keys()):\n    image_path = os.path.join(doremi_dir + \"Images\", img_name)\n    img = Image.open(image_path)\n    width = img.width\n    # Crop image to vertical slice (top to bottom)\n    cropped = img.crop((0, top, width, bottom))\n    image_notes = image_to_notes[img_name]\n    for note in image_notes:\n        note.top -= max_upper\n    cropped_note_pairs.append((cropped, image_notes))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:10:30.716647Z","iopub.execute_input":"2025-04-22T04:10:30.716924Z","iopub.status.idle":"2025-04-22T04:10:32.876704Z","shell.execute_reply.started":"2025-04-22T04:10:30.716900Z","shell.execute_reply":"2025-04-22T04:10:32.876044Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from PIL import Image, ImageDraw\n\n#pprint.pprint(cropped_note_pairs)\nfor i in range(200):\n    test_image, notes = cropped_note_pairs[i]\n    draw = ImageDraw.Draw(test_image)\n\n    # Draw bounding boxes on the image\n    counter = 0;\n    for note in notes:\n        draw.rectangle(note.create_box(), outline=\"red\", width=2)\n        draw.text(note.create_box()[1], str(counter), fill=\"black\")\n        counter += 1\n    display(test_image)\n    width, height = test_image.size\n    print(width)\n    print(height)\n\nsample_cropped_img = test_image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:12:37.982095Z","iopub.execute_input":"2025-04-22T04:12:37.982447Z","iopub.status.idle":"2025-04-22T04:12:39.710512Z","shell.execute_reply.started":"2025-04-22T04:12:37.982424Z","shell.execute_reply":"2025-04-22T04:12:39.709757Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt \n\ndef remove_staff_lines(binary_img: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Detect and remove horizontal staff lines from the binary image input. \n\n    Output: \n        np.ndarray: a binary image with horizontal staff lines removed.\n    \"\"\"\n    horizontal_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (40,1))\n    detected_stafflines = cv2.morphologyEx(binary_img, cv2.MORPH_OPEN, horizontal_kernel, iterations=1)\n    return cv2.subtract(binary_img, detected_stafflines)\n\ndef extract_noteheads(img_path, debug: bool) -> np.ndarray:\n    # convert image to grayscale\n    # img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n\n    # testing with 'cropped_img' variable\n    gray_img = img_path.convert('L')\n    gray_img = np.array(gray_img, dtype=np.uint8)\n\n    # assign all pixel values higher than 127 to 255 and others to 0\n    _, binary_img = cv2.threshold(gray_img, 127, 255, cv2.THRESH_BINARY_INV)\n\n    # remove staff lines\n    processed_img = remove_staff_lines(binary_img)\n\n    # morphological closing to connect noteheads\n    kernel_1 = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3,3))\n    morphed = cv2.morphologyEx(processed_img, cv2.MORPH_CLOSE, kernel_1, iterations=1)\n\n    # erode away isolated pixels that are not note heads\n    kernel_2 = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3,3))\n    morphed = cv2.morphologyEx(morphed, cv2.MORPH_OPEN, kernel_2, iterations=1)\n\n    # find contours\n    contours, _ = cv2.findContours(morphed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n    # filter out shapes that are not noteheads by checking how round the white area is\n    notehead_mask = np.zeros_like(morphed)\n    for item in contours:\n        x, y, w, h = cv2.boundingRect(item)\n        \n        if w<h: continue\n            \n        area = cv2.contourArea(item)\n        perim = cv2.arcLength(item, True)\n\n        if perim == 0: continue \n\n        # compute circularity \n        circ = 4 * np.pi * area / (perim * perim)\n\n        if 0.6 < circ < 1.0:\n            cv2.drawContours(notehead_mask, [item], -1, 255, -1)\n\n    morphed = notehead_mask\n\n    if debug:\n        plt.figure(figsize=(6,6))\n        plt.imshow(morphed, cmap='gray')\n        plt.title(\"Morphed (staff lines out, noteheads closed)\")\n        plt.axis('off')\n        plt.show()\n\n    return morphed, notehead_mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:10:34.723191Z","iopub.execute_input":"2025-04-22T04:10:34.723530Z","iopub.status.idle":"2025-04-22T04:10:34.734379Z","shell.execute_reply.started":"2025-04-22T04:10:34.723503Z","shell.execute_reply":"2025-04-22T04:10:34.733367Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i in range(10):\n    test_image, notes = cropped_note_pairs[i]    \n    morphed, mask = extract_noteheads(test_image, False)\n\n    # Plot side by side\n    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n    axes[0].imshow(np.array(test_image), cmap='gray')\n    axes[0].set_title('Original Image')\n    axes[0].axis('off')\n    \n    axes[1].imshow(np.array(morphed), cmap='gray')\n    axes[1].set_title('Morphed Image')\n    axes[1].axis('off')\n    \n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:18:47.088793Z","iopub.execute_input":"2025-04-22T04:18:47.089451Z","iopub.status.idle":"2025-04-22T04:18:50.015643Z","shell.execute_reply.started":"2025-04-22T04:18:47.089429Z","shell.execute_reply":"2025-04-22T04:18:50.014882Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cropping Noteheads\n\nGiven the bounding boxes drawn on the morphed images, we can take these values and overlay them on the original images to get the cropped images of the non-morphed noteheads.","metadata":{}},{"cell_type":"markdown","source":"# Building Dataframe\n\nHere we will build the dataframe to be passed into our CNN model with the data being individual noteheads and the labels being the one hot encoded class names. ","metadata":{}},{"cell_type":"markdown","source":"# CNN Architecture\n\nHere, we will pass in the cropped noteheads into our CNN model to do multiclass classification, determining if the notes are `noteheadBlack`, `noteheadHalf`, and `noteheadWhole`.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n# find image dimensions per note head \n# IMAGE_SHAPE = ()\n\ncnn_model = Sequential([\n    # First Convolutional Layer\n    Conv2D(32, (3,3), activation='relu', input_shape=IMAGE_SHAPE),\n    BatchNormalization(),\n    MaxPooling2D((2,2)),\n    \n    # Second Convolutional layer\n    Conv2D(64, (3,3), activation='relu'),\n    BatchNormalization(),\n    MaxPooling2D((2,2)),\n    \n    # Fully connect classifier\n    Flatten(),\n    Dense(128, activation='relu'),\n    Dropout(0.5),\n\n    # For single label classification (one label per image)\n    Dense(73, activation='softmax')\n])\n\ncnn_model.summary()\ncnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:10:35.475573Z","iopub.execute_input":"2025-04-22T04:10:35.475799Z","iopub.status.idle":"2025-04-22T04:10:35.525544Z","shell.execute_reply.started":"2025-04-22T04:10:35.475781Z","shell.execute_reply":"2025-04-22T04:10:35.524427Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#","metadata":{}}]}