{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Defining Classes"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-04-29T03:00:51.912102Z","iopub.status.busy":"2025-04-29T03:00:51.911700Z","iopub.status.idle":"2025-04-29T03:00:51.917665Z","shell.execute_reply":"2025-04-29T03:00:51.916291Z","shell.execute_reply.started":"2025-04-29T03:00:51.912071Z"},"trusted":true},"outputs":[],"source":["doremi_dir = \"/kaggle/input/doremi-raw-dataset/DoReMi_v1/\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-04-29T03:00:51.920613Z","iopub.status.busy":"2025-04-29T03:00:51.919804Z","iopub.status.idle":"2025-04-29T03:00:51.944374Z","shell.execute_reply":"2025-04-29T03:00:51.943376Z","shell.execute_reply.started":"2025-04-29T03:00:51.920583Z"},"trusted":true},"outputs":[],"source":["class Staff:\n","    def __init__(self, center: float, upper_bound: int, lower_bound: int, unit_size: float, track: int):\n","        self.center = center\n","        self.upper_bound = upper_bound\n","        self.lower_bound = lower_bound\n","        self.unit_size = unit_size\n","        self.track = track\n","\n","    def __repr__(self):\n","        return (\n","            \"Staff {\\n\"\n","            f\"    Center: {self.center:.1f}\\n\"\n","            f\"    Upper bound: {self.upper_bound}\\n\"\n","            f\"    Lower bound: {self.lower_bound}\\n\"\n","            f\"    Unit size: {self.unit_size:.1f}\\n\"\n","            f\"    Track: {self.track}\\n\"\n","            \"}\"\n","        )\n","        \n","class Note:\n","    def __init__(self, class_name: str, top: int, left: int, width: int, height: int, img_name: str):\n","        self.class_name = class_name\n","        self.top = top\n","        self.left = left\n","        self.width = width\n","        self.height = height\n","        self.image = img_name\n","    def gather_coordinates(self):\n","        return [self.top, self.left, self.width, self.height]\n","    def create_box(self):\n","        return [(self.left, self.top), (self.left + self.width, self.top + self.height)]"]},{"cell_type":"markdown","metadata":{},"source":["# Preprocessing Images"]},{"cell_type":"markdown","metadata":{},"source":["## Sample Image"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-04-29T03:00:51.945625Z","iopub.status.busy":"2025-04-29T03:00:51.945296Z","iopub.status.idle":"2025-04-29T03:00:52.020602Z","shell.execute_reply":"2025-04-29T03:00:52.019509Z","shell.execute_reply.started":"2025-04-29T03:00:51.945598Z"},"trusted":true},"outputs":[],"source":["from PIL import Image\n","\n","img_path = doremi_dir + 'Images/Delius - String Quartet mvt III-003.png'\n","image = Image.open(img_path)\n","file_path = doremi_dir + 'Parsed_by_page_omr_xml/Parsed_Alkan - Posement-layout-0-muscima_Page_1.xml'\n","with open(file_path, 'r') as file:\n","    content = file.read(200)\n","    print(content)\n","image"]},{"cell_type":"markdown","metadata":{},"source":["## Preprocessing Stafflines"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-04-29T03:44:19.966914Z","iopub.status.busy":"2025-04-29T03:44:19.966599Z","iopub.status.idle":"2025-04-29T03:44:20.045787Z","shell.execute_reply":"2025-04-29T03:44:20.044466Z","shell.execute_reply.started":"2025-04-29T03:44:19.966892Z"},"trusted":true},"outputs":[],"source":["import re\n","import pprint\n","import os\n","\n","def natural_key(string):\n","    return [int(s) if s.isdigit() else s.lower() for s in re.split('(\\d+)', string)]\n","\n","# Sorts Images and Parsed_by_page_omr_xml directories\n","image_list = os.listdir(doremi_dir + \"Images\")\n","omr_xml_list =  os.listdir(doremi_dir + \"Parsed_by_page_omr_xml\")\n","\n","image_list.sort(key=natural_key)\n","omr_xml_list.sort(key=natural_key)\n","\n","# Matches each .png file to their respective omr_xml file\n","image_to_omr = {img: omr for img, omr in zip(image_list, omr_xml_list)}\n","\n","def extract_staffs_from_image(cropped_image, padding_multiplier=4.5):\n","    image_gray = cropped_image.convert(\"L\")\n","    image_np = np.array(image_gray)\n","    _, binary = cv2.threshold(image_np, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n","    horizontal_projection = np.sum(binary, axis=1)\n","    peaks, _ = find_peaks(horizontal_projection, height=np.max(horizontal_projection) * 0.5, distance=5)\n","\n","    grouped_staffs = []\n","    for i in range(0, len(peaks) - 4, 5):\n","        group = peaks[i:i+5]\n","        grouped_staffs.append(group)\n","\n","    staff_objects = []\n","    for i, group in enumerate(grouped_staffs):\n","        y_center = float(np.mean(group))\n","        unit_size = float(np.mean(np.diff(group)))\n","        padding = int(unit_size * padding_multiplier)\n","\n","        y_min = int(np.min(group) - padding)\n","        y_max = int(np.max(group) + padding)\n","        track = i % 2 + 1\n","\n","        staff = Staff(\n","            center=y_center,\n","            upper_bound=y_min,\n","            lower_bound=y_max,\n","            unit_size=unit_size,\n","            track=track\n","        )\n","        staff_objects.append(staff)\n","\n","    return staff_objects"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-04-29T03:00:52.109576Z","iopub.status.busy":"2025-04-29T03:00:52.109260Z","iopub.status.idle":"2025-04-29T03:00:57.389209Z","shell.execute_reply":"2025-04-29T03:00:57.387862Z","shell.execute_reply.started":"2025-04-29T03:00:52.109555Z"},"trusted":true},"outputs":[],"source":["from PIL import Image\n","import os\n","import numpy as np\n","import cv2 \n","from scipy.signal import find_peaks\n","\n","images = []\n","staff_data_per_image = {}\n","\n","selected_files = [\n","    \"Delius - String Quartet mvt III\",\n","    \"Schumann - String Quartet 1 mvt 3\",\n","    \"Webern - Variations Op 27 III\"]\n","\n","for image_name in image_list:\n","    if not any(image_name.startswith(prefix) for prefix in selected_files):\n","        continue\n","\n","    image_path = os.path.join(doremi_dir, \"Images\", image_name)\n","    try:\n","        cropped_image = Image.open(image_path)\n","        staff_objects = extract_staffs_from_image(cropped_image)\n","\n","        images.append(image_name)\n","        staff_data_per_image[image_name] = staff_objects \n","\n","        if len(images) == 200:\n","            break\n","    except Exception as e:\n","        print(f\"Skipping {image_name} due to error: {e}\")\n","        continue\n","\n","# Confirm results\n","print(f\"\\nFound {len(images)} images.\\n\") \n","print(images)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-04-29T03:00:57.390634Z","iopub.status.busy":"2025-04-29T03:00:57.390282Z","iopub.status.idle":"2025-04-29T03:01:02.882615Z","shell.execute_reply":"2025-04-29T03:01:02.881384Z","shell.execute_reply.started":"2025-04-29T03:00:57.390604Z"},"trusted":true},"outputs":[],"source":["max_height = 0\n","max_image_name = None\n","max_upper = None\n","max_lower = None\n","\n","IMG_TO_STAFF = {}\n","\n","for image_name in images:\n","    image_path = os.path.join(doremi_dir + \"Images\", image_name)\n","    img = Image.open(image_path)\n","\n","    staff_objects = extract_staffs_from_image(img)\n","    n = len(staff_objects)\n","    if max_upper == None or staff_objects[0].upper_bound < max_upper:\n","        max_upper = staff_objects[0].upper_bound\n","        \n","    if max_lower == None or staff_objects[n - 1].lower_bound > max_lower:\n","        max_lower = staff_objects[n - 1].lower_bound\n","\n","    IMG_TO_STAFF[image_name] = staff_objects\n","\n","print(f\"Upper bound: {max_upper}\")\n","print(f\"Lower bound: {max_lower}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-04-29T03:01:02.884236Z","iopub.status.busy":"2025-04-29T03:01:02.883897Z","iopub.status.idle":"2025-04-29T03:01:04.351699Z","shell.execute_reply":"2025-04-29T03:01:04.350500Z","shell.execute_reply.started":"2025-04-29T03:01:02.884204Z"},"trusted":true},"outputs":[],"source":["import cv2\n","from scipy.signal import find_peaks\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","image_gray = image.convert(\"L\")\n","image_np = np.array(image_gray)\n","\n","_, binary = cv2.threshold(image_np, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n","\n","horizontal_projection = np.sum(binary, axis=1)\n","\n","peaks, _ = find_peaks(horizontal_projection, height=np.max(horizontal_projection) * 0.5, distance=5)\n","\n","grouped_staffs = []\n","for i in range(0, len(peaks) - 4, 5):\n","    group = peaks[i:i+5]\n","    grouped_staffs.append(group)\n","\n","image_color = np.stack([image_np]*3, axis=-1)\n","\n","staff_bounds = []\n","staff_bounds = []\n","staff_objects = []\n","\n","for i, group in enumerate(grouped_staffs):\n","    y_center = float(np.mean(group))\n","    unit_size = float(np.mean(np.diff(group)))\n","    padding = int(unit_size * 3.5)\n","\n","    y_min = int(np.min(group) - padding)\n","    y_max = int(np.max(group) + padding)\n","    track = i % 2 + 1\n","\n","    staff = Staff(\n","        center=y_center,\n","        upper_bound=y_min,\n","        lower_bound=y_max,\n","        unit_size=unit_size,\n","        track=track\n","    )\n","    staff_objects.append(staff)\n","    staff_bounds.append((y_min, y_max))\n","\n","    cv2.rectangle(image_color, (0, y_min), (image_np.shape[1], y_max), (0, 255, 0), 2)\n","    print(staff)\n","\n","\n","plt.figure(figsize=(20, 20))\n","plt.imshow(image_color)\n","plt.axis(\"off\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Preprocessing Notes"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-04-29T03:01:04.353595Z","iopub.status.busy":"2025-04-29T03:01:04.353268Z","iopub.status.idle":"2025-04-29T03:01:04.927064Z","shell.execute_reply":"2025-04-29T03:01:04.925812Z","shell.execute_reply.started":"2025-04-29T03:01:04.353572Z"},"trusted":true},"outputs":[],"source":["import xml.etree.ElementTree as ET\n","\n","IMAGE_TO_NOTES = {}\n","for key in images:\n","# Parse the XML file\n","    xml_path = os.path.join(doremi_dir, \"Parsed_by_page_omr_xml\", image_to_omr[key])\n","    tree = ET.parse(xml_path) \n","    root = tree.getroot()\n","# Define target classnames\n","    target_classes = {'noteheadBlack', 'noteheadHalf', 'noteheadWhole'}\n","\n","# Collect matching nodes\n","    matching_nodes = []\n","\n","# Loop through all <Node> elements\n","    for node in root.findall('.//Node'):\n","        class_name = node.find('ClassName')\n","        if class_name is not None and class_name.text in target_classes:\n","            matching_nodes.append(node)\n","\n","# Print results\n","    final_notes = []\n","    for match in matching_nodes:\n","        note = Note(match.find(\"ClassName\").text, \n","                    int(match.find(\"Top\").text), \n","                    int(match.find(\"Left\").text), \n","                    int(match.find(\"Width\").text), \n","                    int(match.find(\"Height\").text), \n","                    key\n","                   )\n","        final_notes.append(note)\n","        # print(note.image)\n","    IMAGE_TO_NOTES[key] = final_notes"]},{"cell_type":"markdown","metadata":{},"source":["### Cropping Stafflines"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-04-29T03:01:04.928800Z","iopub.status.busy":"2025-04-29T03:01:04.928431Z","iopub.status.idle":"2025-04-29T03:01:05.989738Z","shell.execute_reply":"2025-04-29T03:01:05.988440Z","shell.execute_reply.started":"2025-04-29T03:01:04.928774Z"},"trusted":true},"outputs":[],"source":["CROPPED_NOTE_PAIRS = []\n","for image_name, notes in IMAGE_TO_NOTES.items():\n","    image_path = os.path.join(doremi_dir + \"Images\", image_name)\n","    img = Image.open(image_path)\n","    width = img.width\n","    \n","    # Crop image to vertical slice (top to bottom)\n","    cropped = img.crop((0, max_upper, width, max_lower))\n","    for note in notes:\n","        note.top -= max_upper\n","    CROPPED_NOTE_PAIRS.append((cropped, notes))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-04-29T03:01:05.991251Z","iopub.status.busy":"2025-04-29T03:01:05.990891Z","iopub.status.idle":"2025-04-29T03:01:06.113590Z","shell.execute_reply":"2025-04-29T03:01:06.112399Z","shell.execute_reply.started":"2025-04-29T03:01:05.991231Z"},"trusted":true},"outputs":[],"source":["from PIL import Image, ImageDraw\n","\n","for i in range(5):\n","    test_image, notes = CROPPED_NOTE_PAIRS[i]\n","    img = test_image.copy()  \n","    draw = ImageDraw.Draw(img)\n","\n","    # Draw bounding boxes on the image\n","    counter = 0;\n","    for note in notes:\n","        draw.rectangle(note.create_box(), outline=\"red\", width=2)\n","        draw.text(note.create_box()[1], str(counter), fill=\"black\")\n","        counter += 1\n","    display(img)\n","    width, height = img.size\n","    print(width)\n","    print(height)\n","        \n","test_image, notes = CROPPED_NOTE_PAIRS[0]\n","img = test_image.copy()  \n","sample_cropped_img = test_image"]},{"cell_type":"markdown","metadata":{},"source":["# Extracting Note"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-04-29T03:01:06.115346Z","iopub.status.busy":"2025-04-29T03:01:06.114824Z","iopub.status.idle":"2025-04-29T03:01:06.121734Z","shell.execute_reply":"2025-04-29T03:01:06.120674Z","shell.execute_reply.started":"2025-04-29T03:01:06.115292Z"},"trusted":true},"outputs":[],"source":["import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt \n","\n","def remove_staff_lines(binary_img: np.ndarray) -> np.ndarray:\n","    \"\"\"\n","    Detect and remove horizontal staff lines from the binary image input. \n","\n","    Output: \n","        np.ndarray: a binary image with horizontal staff lines removed.\n","    \"\"\"\n","    horizontal_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (40,1))\n","    detected_stafflines = cv2.morphologyEx(binary_img, cv2.MORPH_OPEN, horizontal_kernel, iterations=1)\n","    return cv2.subtract(binary_img, detected_stafflines)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-04-29T03:01:06.123294Z","iopub.status.busy":"2025-04-29T03:01:06.122947Z","iopub.status.idle":"2025-04-29T03:01:06.146642Z","shell.execute_reply":"2025-04-29T03:01:06.145525Z","shell.execute_reply.started":"2025-04-29T03:01:06.123258Z"},"trusted":true},"outputs":[],"source":["import cv2\n","import numpy as np\n","\n","def selective_hole_fill(binary, min_hole_area, max_hole_area) -> np.ndarray:\n","    \"\"\"\n","    Fill only fully enclosed holes whose area is between min_hole_area and max_hole_area.\n","    \"\"\"\n","    # invert the binary image\n","    inv = cv2.bitwise_not(binary)\n","    \n","    # label connected components in inverted image\n","    num, labels, stats, centroids = cv2.connectedComponentsWithStats(inv, connectivity=8)\n","    \n","    h, w = binary.shape\n","    filled = binary.copy()\n","\n","    # filling step\n","    for lbl in range(1, num): # iterated thru each component\n","        area = stats[lbl, cv2.CC_STAT_AREA] \n","        # only fill holes in the desired size range\n","        if area < min_hole_area or area > max_hole_area:\n","            continue\n","        filled[labels == lbl] = 255\n","\n","    return filled"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-04-29T03:01:06.148090Z","iopub.status.busy":"2025-04-29T03:01:06.147812Z","iopub.status.idle":"2025-04-29T03:01:06.187910Z","shell.execute_reply":"2025-04-29T03:01:06.185716Z","shell.execute_reply.started":"2025-04-29T03:01:06.148069Z"},"trusted":true},"outputs":[],"source":["WIDTH_FACTOR = 0.5\n","HEIGHT_FACTOR = 0.4\n","MIN_AREA = 100 # pixels\n","MAX_AREA = 600\n","\n","MAX_HEIGHT_WIDTH_RATIO = 2.0 # h / w\n","MIN_WIDTH_HEIGHT_RATIO = 1 # w / h \n","ELLIPSE_SIZE = 20.5\n","PAD = 4\n","\n","def extract_noteheads(img, debug: bool) -> np.ndarray:\n","    # testing with 'cropped_img' variable\n","    gray_img = img.convert('L')\n","    gray_img = np.array(gray_img, dtype=np.uint8)\n","\n","    # assign all pixel values higher than 127 to 255 and others to 0\n","    _, binary_img = cv2.threshold(gray_img, 127, 255, cv2.THRESH_BINARY_INV)\n","    \n","    if debug:\n","        plt.figure(figsize=(6,6))\n","        plt.imshow(binary_img, cmap='gray')\n","        plt.title(\"Binary Inverted\")\n","        plt.axis('off')\n","        plt.show()\n","        \n","    # remove staff lines\n","    processed_img = remove_staff_lines(binary_img)\n","\n","    # Performing morphological operations\n","    w_close = max(1, int(round(ELLIPSE_SIZE * 0.2)))\n","    h_close = max(1, int(round(ELLIPSE_SIZE * 0.4)))\n","    close_krn = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (w_close, h_close))\n","    closed = cv2.morphologyEx(processed_img, cv2.MORPH_CLOSE, close_krn)\n","    \n","    filled = selective_hole_fill(\n","        closed, \n","        min_hole_area=150,\n","        max_hole_area=250,\n","        )\n","    \n","    if debug:\n","        plt.figure(figsize=(6,6))\n","        plt.imshow(closed, cmap='gray')\n","        plt.title(\"After Closed\")\n","        plt.axis('off')\n","        plt.show()\n","           \n","    notehead_size = (\n","        int(round(ELLIPSE_SIZE * WIDTH_FACTOR)),\n","        int(round(ELLIPSE_SIZE * HEIGHT_FACTOR))\n","    )\n","    \n","    # erode away isolated pixels that are not note heads\n","    kernel_2 = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, notehead_size)\n","    opened = cv2.morphologyEx(filled, cv2.MORPH_OPEN, kernel_2)\n","\n","    if debug:\n","        plt.figure(figsize=(6,6))\n","        plt.imshow(opened, cmap='gray')\n","        plt.title(\"After Opened\")\n","        plt.axis('off')\n","        plt.show()\n","        \n","    w_close = max(1, int(round(ELLIPSE_SIZE * 0.1)))\n","    h_close = max(1, int(round(ELLIPSE_SIZE * 0.8)))\n","    close_krn = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (w_close, h_close))\n","    closed = cv2.morphologyEx(opened, cv2.MORPH_CLOSE, close_krn)\n","    \n","    if debug:\n","        plt.figure(figsize=(6,6))\n","        plt.imshow(closed, cmap='gray')\n","        plt.title(\"After Second Closed\")\n","        plt.axis('off')\n","        plt.show()\n","        \n","    # find contours\n","    contours, _ = cv2.findContours(closed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","    \n","    if debug:\n","        canvas = cv2.cvtColor(closed, cv2.COLOR_GRAY2BGR)\n","        \n","        cv2.drawContours(canvas, contours, -1, (0,255,0), 1)\n","        \n","        plt.figure(figsize=(6,6))\n","        plt.imshow(canvas[...,::-1])\n","        plt.title(f'{len(contours)} contours found')\n","        plt.axis('off')\n","        plt.show()\n","\n","    # filter out shapes that are not noteheads by checking how round the white area is\n","    notehead_mask = np.zeros_like(opened)\n","    for item in contours:\n","        area = cv2.contourArea(item)\n","        # print(area)\n","        if area < MIN_AREA or area > MAX_AREA:\n","            continue\n","        \n","        x, y, w, h = cv2.boundingRect(item)\n","        \n","        # Drop any blob that's too tall/thin:\n","        if h / float(w) > MAX_HEIGHT_WIDTH_RATIO:\n","            continue\n","\n","        # Drop any blob that's too wide/flat:\n","        if w / float(h) < MIN_WIDTH_HEIGHT_RATIO:\n","            continue\n","\n","        cv2.drawContours(notehead_mask, [item], -1, 255, -1)\n","\n","    morphed = notehead_mask\n","\n","    if debug:\n","        plt.figure(figsize=(6,6))\n","        plt.imshow(morphed, cmap='gray')\n","        plt.title(\"Morphed (staff lines out, noteheads closed)\")\n","        plt.axis('off')\n","        plt.show()\n","        \n","    final_contours, _ = cv2.findContours(\n","        notehead_mask,\n","        cv2.RETR_EXTERNAL,\n","        cv2.CHAIN_APPROX_SIMPLE\n","    )\n","\n","    return morphed, final_contours"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-04-29T03:01:06.193479Z","iopub.status.busy":"2025-04-29T03:01:06.193109Z","iopub.status.idle":"2025-04-29T03:01:08.298929Z","shell.execute_reply":"2025-04-29T03:01:08.297884Z","shell.execute_reply.started":"2025-04-29T03:01:06.193443Z"},"trusted":true},"outputs":[],"source":["test_image, notes = CROPPED_NOTE_PAIRS[5]\n","morphed_img, contours = extract_noteheads(test_image, True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-04-29T03:01:08.300172Z","iopub.status.busy":"2025-04-29T03:01:08.299800Z","iopub.status.idle":"2025-04-29T03:01:08.306211Z","shell.execute_reply":"2025-04-29T03:01:08.305393Z","shell.execute_reply.started":"2025-04-29T03:01:08.300145Z"},"trusted":true},"outputs":[],"source":["def display_image(image, title=\"Image\"):\n","    plt.figure(figsize=(10, 8))\n","    plt.imshow(image, cmap='gray')\n","    plt.title(title)\n","    plt.axis('off')\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-04-29T03:01:08.308580Z","iopub.status.busy":"2025-04-29T03:01:08.307291Z","iopub.status.idle":"2025-04-29T03:01:08.334918Z","shell.execute_reply":"2025-04-29T03:01:08.333676Z","shell.execute_reply.started":"2025-04-29T03:01:08.308546Z"},"trusted":true},"outputs":[],"source":["def get_roi_coords(mask):\n","    \"\"\"\n","    Compute padded bounding-box coordinates for a binary mask.\n","    Returns (x0, y0, x1, y1).\n","    \"\"\"\n","    x, y, w, h = cv2.boundingRect(mask)\n","    # make sure we don't go out of bounds\n","    x0 = max(0, x - PAD) \n","    y0 = max(0, y - PAD)\n","    x1 = min(mask.shape[1], x + w + PAD)\n","    y1 = min(mask.shape[0], y + h + PAD)\n","    return x0, y0, x1, y1\n","\n","def split_horizontal_blob(mask, debug=False):\n","    \"\"\"\n","    Split a single-blob mask horizontally through its vertical midpoint.\n","    Returns [top_half, bottom_half] or [mask] if one half is empty.\n","    \"\"\"\n","    # Compute bounding box\n","    x, y, w, h = cv2.boundingRect(mask)\n","    mid_y = y + h // 2 # calculating the midway point\n","\n","    # Empty masks\n","    top = np.zeros_like(mask)\n","    bottom = np.zeros_like(mask)\n","\n","    # Slicing within bbox\n","    top[y:mid_y, x:x+w] = mask[y:mid_y, x:x+w]\n","    bottom[mid_y:y+h, x:x+w] = mask[mid_y:y+h, x:x+w]\n","    pieces = [top, bottom]\n","\n","    # If one half is empty, return original. This means that either mask is completely all black\n","    if cv2.countNonZero(top) == 0 or cv2.countNonZero(bottom) == 0:\n","        return [mask]\n","\n","    # Creating a kernel\n","    radius = 5\n","    ksize = 2 * radius + 1\n","    kern  = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (ksize, ksize))\n","\n","    # apply opening to each piece to erode straight edges, then dilate back\n","    # basically erosding away the straight edges and then dilating again to give it its shape back\n","    rounded = [cv2.morphologyEx(p, cv2.MORPH_OPEN, kern) for p in pieces]\n","\n","    if debug:\n","        for i, m in enumerate(rounded):\n","            x0, y0, x1, y1 = get_roi_coords(mask)\n","            plt.figure(figsize=(3,3))\n","            plt.imshow(m[y0:y1, x0:x1], cmap='gray')\n","            plt.title(f'Rounded Piece #{i}')\n","            plt.axis('off')\n","        plt.show()\n","\n","    return rounded"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-04-29T03:01:08.336778Z","iopub.status.busy":"2025-04-29T03:01:08.336007Z","iopub.status.idle":"2025-04-29T03:01:08.371699Z","shell.execute_reply":"2025-04-29T03:01:08.370755Z","shell.execute_reply.started":"2025-04-29T03:01:08.336746Z"},"trusted":true},"outputs":[],"source":["def split_blob(mask, debug=False):\n","    \"\"\"\n","    Split a large binary mask into smaller ones using watershed,until each piece's nonzero area <= MAX_AREA.\n","    Returns a list of mask pieces.\n","    \"\"\"\n","    # If this mask is already small enough, return it. Error checking bascially\n","    area = cv2.countNonZero(mask)\n","    if area <= MAX_AREA:\n","        if debug:\n","            x0, y0, x1, y1 = get_roi_coords(mask)\n","            plt.figure(figsize=(4,4))\n","            plt.imshow(mask[y0:y1, x0:x1], cmap='gray', vmin=0, vmax=255, interpolation='nearest')\n","            plt.title(f'Final Piece (area={area})')\n","            plt.axis('off')\n","        return [mask]\n","\n","    # 1) Compute distance transform\n","    # Computes shortest distance from foreground pixel to the background pixel. Identifying the center regions of the object\n","    dist = cv2.distanceTransform(mask, cv2.DIST_L2, 5)\n","    if debug:\n","        x0, y0, x1, y1 = get_roi_coords(mask)\n","\n","        plt.figure(figsize=(4,4))\n","        plt.imshow(dist[y0:y1, x0:x1], cmap='viridis', interpolation='nearest')\n","        plt.title('Distance Transform')\n","        plt.axis('off')\n","\n","    # 2) Get sure foreground peaks (areas that are confidently park of the foreground, deep enough within the object)\n","    # Extracting deepest pixels from background boundary, 40% of blob distance\n","    thresh_val = 0.6 * dist.max()\n","    _, fg = cv2.threshold(dist, thresh_val, 255, cv2.THRESH_BINARY)\n","    fg = fg.astype(np.uint8) # converting to be used in OpenCV\n","    if debug:\n","        x0, y0, x1, y1 = get_roi_coords(mask)\n","\n","        plt.figure(figsize=(4,4))\n","        plt.imshow(fg[y0:y1, x0:x1], cmap='gray', vmin=0, vmax=255, interpolation='nearest')\n","        plt.title('Sure Foreground Peaks')\n","        plt.axis('off')\n","\n","    # 3) Create markers for watershed\n","    num_components, markers = cv2.connectedComponents(fg)\n","    if num_components < 2: # only one component in the foreground\n","        return [mask]\n","    markers = markers + 1 # 0 is reserved for the background\n","    unknown = cv2.subtract(mask, fg) # marking the unknown region\n","    markers[unknown == 255] = 0 # uncertain pixels\n","    \n","    if debug:\n","        x0, y0, x1, y1 = get_roi_coords(mask)\n","\n","        plt.figure(figsize=(4,4))\n","        plt.imshow(markers[y0:y1, x0:x1], cmap='tab20', interpolation='nearest')\n","        plt.title('Watershed Markers')\n","        plt.axis('off')\n","\n","    # 4) Apply watershed\n","    color = cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR) # watershed needs a colored image\n","    cv2.watershed(color, markers) # running watershed, flooding from seed labels\n","    \n","    if debug:\n","        boundary_overlay = color.copy()\n","        boundary_overlay[markers == -1] = (0,0,255)\n","        x0, y0, x1, y1 = get_roi_coords(mask)\n","\n","        plt.figure(figsize=(4,4))\n","        plt.imshow(boundary_overlay[y0:y1, x0:x1][..., ::-1], interpolation='nearest')\n","        plt.title('Watershed Boundaries ')\n","        plt.axis('off')\n","\n","    # 5) Extract pieces\n","    pieces = []\n","    for lbl in range(2, num_components+1):\n","        piece = np.zeros_like(mask)\n","        piece[markers == lbl] = 255\n","        if cv2.countNonZero(piece) > 0:\n","            if debug:\n","                x0, y0, x1, y1 = get_roi_coords(piece)\n","                plt.figure(figsize=(4,4))\n","                plt.imshow(piece[y0:y1, x0:x1], cmap='gray', vmin=0, vmax=255, interpolation='nearest')\n","                plt.title(f'Piece {lbl-1}')\n","                plt.axis('off')\n","            pieces.append(piece)\n","\n","    # If watershed didn't split, return original\n","    if not pieces:\n","        return [mask]\n","        \n","    aspect_thresh = 1.2 # height has to be 120% of width, if its greater, that means there are two notes on top of each other\n","    pieces = []\n","    \n","    for lbl in range(2, num_components+1):\n","        piece = np.zeros_like(mask)\n","        piece[markers == lbl] = 255\n","        if cv2.countNonZero(piece) > 0:\n","            # checking aspect ratio\n","            x,y,w,h = cv2.boundingRect(piece)\n","                \n","            if h > w * aspect_thresh or h * aspect_thresh > w:\n","                if True:\n","                    x0, y0, x1, y1 = get_roi_coords(piece)\n","                    plt.figure(figsize=(4,4))\n","                    plt.imshow(piece[y0:y1, x0:x1], cmap='gray', vmin=0, vmax=255, interpolation='nearest')\n","                    plt.title(f'Piece {lbl-1} height: {h}, width: {w}')\n","                    plt.axis('off')\n","                # split horizontally if its too tall or too wide\n","                sub = split_horizontal_blob(piece, True)\n","                pieces.extend(sub)\n","            else:\n","                if debug:\n","                    x0, y0, x1, y1 = get_roi_coords(piece)\n","                    plt.figure(figsize=(4,4))\n","                    plt.imshow(piece[y0:y1, x0:x1], cmap='gray', vmin=0, vmax=255, interpolation='nearest')\n","                    plt.title(f'Piece {lbl-1} (zoomed)'); plt.axis('off')\n","                pieces.append(piece)\n","    return pieces"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-04-29T04:13:27.470420Z","iopub.status.busy":"2025-04-29T04:13:27.470041Z","iopub.status.idle":"2025-04-29T04:13:27.486821Z","shell.execute_reply":"2025-04-29T04:13:27.485783Z","shell.execute_reply.started":"2025-04-29T04:13:27.470397Z"},"trusted":true},"outputs":[],"source":["import cv2\n","import numpy as np\n","id_num = 0\n","def refine_and_fit_ellipses(gray_img, notehead_mask, contours,\n","                            iou_thresh=0.75,\n","                            min_area=150,\n","                            min_ellipse_area=300,\n","                            min_minor_axis=10,\n","                            min_axis_ratio=0.5,\n","                            min_area_ratio=0.6,\n","                            MAX_AREA_ratio=1.3,\n","                            min_solidity=0.9,\n","                            debug=False,):\n","    \"\"\"\n","    Returns two lists:\n","     - accepted: [( (cx,cy), (MA,ma), angle ), ... ]\n","     - rejected: [ ( (cx,cy), reason_string ), ... ]\n","    \"\"\"\n","    H, W = gray_img.shape\n","    accepted, rejected = [], []\n","    global id_num\n","    for cnt in contours:\n","        # Computing the raw area\n","        rawA = cv2.contourArea(cnt)\n","        blob = np.zeros_like(notehead_mask)\n","        cv2.drawContours(blob, [cnt], -1, 255, -1)\n","        \n","        if debug:\n","            x0, y0, x1, y1 = get_roi_coords(blob)\n","            \n","            plt.figure(figsize=(4,4))\n","            plt.imshow(blob[y0:y1, x0:x1],\n","                       cmap='gray',\n","                       vmin=0, vmax=255,\n","                       interpolation='nearest')\n","            plt.title(\"Blob mask\")\n","            plt.axis('off')\n","\n","        # If raw area is bigger than the max area, split into multple blobs\n","        if rawA > MAX_AREA:\n","            submasks = split_blob(blob, False)\n","        else:\n","            submasks = [blob]\n","\n","        for piece in submasks:\n","            # extracting pieces from the submask and adding PADding \n","            x0, y0, x1, y1 = get_roi_coords(piece)\n","\n","            roi_gray = gray_img[y0:y1, x0:x1]\n","            roi_mask = piece[y0:y1, x0:x1]\n","            \n","            if debug:\n","                plt.figure(figsize=(4,4))\n","                plt.imshow(roi_gray,\n","                           cmap='gray',\n","                           vmin=0, vmax=255,\n","                           interpolation='nearest')\n","                plt.title(\"ROI gray\")\n","                plt.axis('off')\n","                \n","                plt.figure(figsize=(4,4))\n","                plt.imshow(roi_mask,\n","                           cmap='gray',\n","                           vmin=0, vmax=255,\n","                           interpolation='nearest')\n","                plt.title(\"ROI mask\")\n","                plt.axis('off')\n","                \n","            # finding the local contours within the mask\n","            local_cnts, _ = cv2.findContours(\n","            roi_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n","            )\n","\n","            # Run 9 total tests\n","            for lc in local_cnts:\n","                area = cv2.contourArea(lc)\n","                cx_loc, cy_loc = cv2.boundingRect(lc)[:2]\n","                (cx_loc, cy_loc), (MA, ma), ang = cv2.fitEllipse(lc)\n","                note_metadata = ((cx_loc+x0, cy_loc+y0),(MA,ma),ang)\n","                \n","                # 1) raw-area floor\n","                if area < min_area or area > MAX_AREA:\n","                    rejected.append(note_metadata)\n","                    continue\n","\n","                # 2) need at least 5 points to fit an ellipse\n","                if len(lc) < 5:\n","                    rejected.append(note_metadata)\n","                    continue\n","\n","                # 4) ellipse-area floor\n","                ell_area = np.pi*(MA/2)*(ma/2)\n","                if ell_area < min_ellipse_area:\n","                    rejected.append(note_metadata)\n","                    continue\n","\n","                # 5) minor-axis floor\n","                if min(MA, ma) < min_minor_axis:\n","                    rejected.append(note_metadata)\n","                    continue\n","\n","                # 6) axis-ratio\n","                axis_r = min(MA,ma)/max(MA,ma)\n","                if axis_r < min_axis_ratio:\n","                    rejected.append(note_metadata)\n","                    continue\n","\n","                # 7) area-ratio\n","                area_r = area/ell_area\n","                if not (min_area_ratio <= area_r <= MAX_AREA_ratio):\n","                    rejected.append(note_metadata)\n","                    continue\n","\n","                # if we get here, it’s a keeper\n","                accepted.append((note_metadata, id_num))\n","                id_num+=1\n","\n","    return accepted, rejected"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-04-29T04:13:29.925479Z","iopub.status.busy":"2025-04-29T04:13:29.925136Z","iopub.status.idle":"2025-04-29T04:13:42.013843Z","shell.execute_reply":"2025-04-29T04:13:42.012730Z","shell.execute_reply.started":"2025-04-29T04:13:29.925457Z"},"trusted":true},"outputs":[],"source":["from tqdm import tqdm\n","debug = False\n","ACCEPTED_NOTES = []\n","for test_img, notes in tqdm(CROPPED_NOTE_PAIRS, desc=\"Extracting noteheads\", unit=\"pair\"):    \n","    # 1) Make image gray & binary\n","    if not isinstance(test_img, np.ndarray):\n","        gray = np.array(test_img.convert('L'), dtype=np.uint8)\n","    else:\n","        gray = test_img.astype(np.uint8)\n","    \n","    _, binary = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY_INV)\n","    \n","    # 2) Extract noteheads\n","    morphed, contours = extract_noteheads(\n","        test_img,\n","        debug=False\n","    )\n","    \n","    # 3) Visualize the cleaned contours\n","    if debug:\n","        canvas = cv2.cvtColor(binary, cv2.COLOR_GRAY2BGR)\n","        cv2.drawContours(canvas, contours, -1, (0,255,0), 2)\n","        plt.figure(figsize=(6,6))\n","        plt.imshow(canvas[..., ::-1])\n","        plt.title(f\"Cleaned Image {i}: {len(contours)} contours\")\n","        plt.axis('off')\n","        plt.show()\n","        \n","    # 4) Now call the new fitter\n","    accepted, rejected = refine_and_fit_ellipses(\n","        gray_img = gray,\n","        notehead_mask = morphed,\n","        contours = contours,\n","        min_area = MIN_AREA,\n","        debug = False\n","    )\n","    ACCEPTED_NOTES.append((gray, accepted))\n","    if debug:\n","        print(f\"Accepted: {len(accepted)} ellipses\")\n","        print(f\"Rejected: {len(rejected)} blobs\")\n","        \n","        # 5) Overlay the kept ellipses on your original\n","        canvas2 = cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR)\n","        for (cx,cy), (MA,ma), ang in accepted:\n","            cv2.ellipse(\n","                canvas2,\n","                (int(round(cx)), int(round(cy))),\n","                (int(round(MA/2)), int(round(ma/2))),\n","                angle=ang,\n","                startAngle=0,\n","                endAngle=360,\n","                color=(0,0,255),\n","                thickness=2\n","            )\n","        \n","        plt.figure(figsize=(8,8), dpi=200)\n","        plt.imshow(canvas2[..., ::-1])\n","        plt.title(f\"Accepted Ellipses: {len(accepted)}\")\n","        plt.axis('off')\n","        plt.show()\n","         \n","        canvas2 = cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR)\n","        for (cx,cy), (MA,ma), ang in rejected:\n","            cv2.ellipse(\n","                canvas2,\n","                (int(round(cx)), int(round(cy))),\n","                (int(round(MA/2)), int(round(ma/2))),\n","                angle=ang,\n","                startAngle=0,\n","                endAngle=360,\n","                color=(0,0,255),\n","                thickness=2\n","            )\n","            \n","        plt.figure(figsize=(8,8), dpi=200)\n","        plt.imshow(canvas2[..., ::-1])\n","        plt.title(f\"Rejected Ellipses: {len(rejected)}\")\n","        plt.axis('off')\n","        plt.show()\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-04-29T04:44:04.260216Z","iopub.status.busy":"2025-04-29T04:44:04.259916Z","iopub.status.idle":"2025-04-29T04:44:04.410087Z","shell.execute_reply":"2025-04-29T04:44:04.408730Z","shell.execute_reply.started":"2025-04-29T04:44:04.260197Z"},"trusted":true},"outputs":[],"source":["debug = False\n","TARGET_SIZE = (39, 23)\n","\n","def crop_pad_notes(gray_img, accepted, debug = False):\n","    H, W = gray_img.shape[:2]\n","    bboxes = []\n","    for ((cx, cy), (MA, ma), _), id_num in accepted:\n","        # center X, center Y\n","        # Major Axis - width\n","        # Minor axis - height\n","        w = int(round(MA))\n","        h = int(round(ma))\n","        x0 = max(0, int(round(cx - w/2)))\n","        y0 = max(0, int(round(cy - h/2)))\n","        x1 = min(W, int(round(cx + w/2)))\n","        y1 = min(H, int(round(cy + h/2)))\n","        bboxes.append(((x0, y0, x1, y1), id_num))\n","\n","    # finding universal size\n","    IMAGE_WIDTH, IMAGE_HEIGHT = 0,0\n","    for (x0,y0,x1,y1), _ in bboxes:\n","        IMAGE_WIDTH = max(x1 - x0, IMAGE_WIDTH)\n","        IMAGE_HEIGHT = max(y1 - y0, IMAGE_HEIGHT)\n","\n","    if debug:\n","        print(f\"IMAGE_WIDTH: {IMAGE_WIDTH} IMAGE_HEIGHT: {IMAGE_HEIGHT}\")\n","    \n","    padded_imgs = []\n","    for (x0,y0,x1,y1), id_num in bboxes:\n","        cropped = gray_img[y0:y1, x0:x1]\n","        # print(id_num)\n","        padded_imgs.append((cropped, (x0,y0,x1,y1), id_num))\n","    return padded_imgs\n","\n","IMAGE_DATA = []\n","for img, accepted in tqdm(ACCEPTED_NOTES, desc=\"Cropping noteheads\", unit=\"pair\"):\n","    img_arr = np.array(img)\n","    cropped_images = crop_pad_notes(img_arr, accepted)\n","    for idx, (crop_img, bbox, id_num) in enumerate(cropped_images):\n","        if debug:\n","            plt.figure(figsize=(3,3))\n","            plt.imshow(crop_img, cmap='gray', vmin=0, vmax=255)\n","            plt.title(f\"Cropped Note {idx+1}\")\n","            plt.axis('off')\n","            plt.show()\n","        IMAGE_DATA.append((crop_img, id_num))\n","\n","TRAINING_IMAGES = [(cv2.resize(crop, (TARGET_SIZE[1], TARGET_SIZE[0])), id_num) for crop, id_num in IMAGE_DATA]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-04-29T04:44:05.381405Z","iopub.status.busy":"2025-04-29T04:44:05.381032Z","iopub.status.idle":"2025-04-29T04:44:05.388389Z","shell.execute_reply":"2025-04-29T04:44:05.387190Z","shell.execute_reply.started":"2025-04-29T04:44:05.381381Z"},"trusted":true},"outputs":[],"source":["len(TRAINING_IMAGES)"]},{"cell_type":"markdown","metadata":{},"source":["# Calculating Pitch"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-04-29T03:34:03.061249Z","iopub.status.busy":"2025-04-29T03:34:03.060896Z","iopub.status.idle":"2025-04-29T03:34:03.069229Z","shell.execute_reply":"2025-04-29T03:34:03.067992Z","shell.execute_reply.started":"2025-04-29T03:34:03.061225Z"},"trusted":true},"outputs":[],"source":["def calculate_pitch(cy, staff_center, unit_size):\n","    pitch_step = round(-(cy - staff_center) / (unit_size / 2))\n","    \n","    pitch_names = {\n","    -20: \"C\", -19: \"D\", -18: \"E\", -17: \"F\", -16: \"G\",\n","    -15: \"A\", -14: \"B\", -13: \"C\", -12: \"D\", -11: \"E\",\n","    -10: \"F\",  -9: \"G\",  -8: \"A\",  -7: \"B\",  -6: \"C\",\n","     -5: \"D\",  -4: \"E\",  -3: \"F\",  -2: \"G\",  -1: \"A\",\n","      0: \"B\",   1: \"C\",   2: \"D\",   3: \"E\",   4: \"F\",\n","      5: \"G\",   6: \"A\",   7: \"B\",   8: \"C\",   9: \"D\",\n","     10: \"E\",  11: \"F\",  12: \"G\",  13: \"A\",  14: \"B\",\n","     15: \"C\",  16: \"D\",  17: \"E\",  18: \"F\",  19: \"G\",\n","     20: \"A\"\n","    }\n","\n","    pitch = pitch_names.get(pitch_step, \"Unknown\")\n","    return pitch, pitch_step"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-04-29T03:34:09.274632Z","iopub.status.busy":"2025-04-29T03:34:09.274250Z","iopub.status.idle":"2025-04-29T03:34:09.280754Z","shell.execute_reply":"2025-04-29T03:34:09.279606Z","shell.execute_reply.started":"2025-04-29T03:34:09.274608Z"},"trusted":true},"outputs":[],"source":["def calculate_pitch_multi(y, staffs):\n","    \"\"\"\n","    Pick the correct staff for a note at vertical position y,\n","    then compute its pitch & step using that staff's center & unit_size.\n","    \"\"\"\n","    # 1) find any staff whose bounds contain y\n","    for st in staffs:\n","        if st.upper_bound <= y <= st.lower_bound:\n","            return calculate_pitch(y, st.center, st.unit_size)\n","\n","    # 2) if none matched perfectly, fall back to the *closest* staff center\n","    closest = min(staffs, key=lambda s: abs(s.center - y))\n","    return calculate_pitch(y, closest.center, closest.unit_size)"]},{"cell_type":"markdown","metadata":{},"source":["# Multiclass Classification"]},{"cell_type":"markdown","metadata":{},"source":["## Loading Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-04-29T03:04:00.798339Z","iopub.status.busy":"2025-04-29T03:04:00.797941Z","iopub.status.idle":"2025-04-29T03:04:00.815203Z","shell.execute_reply":"2025-04-29T03:04:00.814177Z","shell.execute_reply.started":"2025-04-29T03:04:00.798289Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","\n","df = pd.read_pickle('/kaggle/input/multiclass-noteheads/df_trunc.pkl')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-04-29T03:16:36.310886Z","iopub.status.busy":"2025-04-29T03:16:36.310192Z","iopub.status.idle":"2025-04-29T03:16:36.323925Z","shell.execute_reply":"2025-04-29T03:16:36.322823Z","shell.execute_reply.started":"2025-04-29T03:16:36.310861Z"},"trusted":true},"outputs":[],"source":["mapping = {\n","    \"noteheadBlack\": 1,\n","    \"noteheadHalf\":  0,\n","    \"noteheadWhole\": 0\n","}\n","\n","df['label_binary'] = df['label'].map(mapping)\n","print(df['label_binary'].value_counts())"]},{"cell_type":"markdown","metadata":{},"source":["## Building CNN \n","\n","This binary classifier will identify if notes are hollow or not"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-04-29T03:16:57.961496Z","iopub.status.busy":"2025-04-29T03:16:57.961054Z","iopub.status.idle":"2025-04-29T03:16:58.057686Z","shell.execute_reply":"2025-04-29T03:16:58.056463Z","shell.execute_reply.started":"2025-04-29T03:16:57.961473Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import (Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization)\n","from tensorflow.keras.callbacks import EarlyStopping\n","\n","IMAGE_SHAPE = (39, 23, 1)\n","\n","binary_model = Sequential([\n","    # First Convolutional Block\n","    Conv2D(32, (3,3), activation='relu', input_shape=IMAGE_SHAPE),\n","    BatchNormalization(),\n","    MaxPooling2D((2,2)),\n","    \n","    # Second Convolutional Block\n","    Conv2D(64, (3,3), activation='relu'),\n","    BatchNormalization(),\n","    MaxPooling2D((2,2)),\n","    \n","    # Classifier\n","    Flatten(),\n","    Dense(128, activation='relu'),\n","    Dropout(0.5),\n","\n","    # Single–unit sigmoid output for binary classification\n","    Dense(1, activation='sigmoid')\n","])\n","\n","binary_model.summary()\n","\n","binary_model.compile(\n","    optimizer='adam',\n","    loss='binary_crossentropy',\n","    metrics=['accuracy']\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-04-29T03:26:11.780099Z","iopub.status.busy":"2025-04-29T03:26:11.779763Z","iopub.status.idle":"2025-04-29T03:26:11.791335Z","shell.execute_reply":"2025-04-29T03:26:11.790394Z","shell.execute_reply.started":"2025-04-29T03:26:11.780069Z"},"trusted":true},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","X = df[\"image\"].tolist()\n","X = np.stack(X) # Stacking arrays into a tensor\n","\n","print(\"Data shape\", X.shape)\n","\n","y = df[\"label_binary\"].values\n","\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X,y,\n","    test_size = 0.2, # 80/20 split\n","    stratify=y, # Need equal parts of each class in each\n","    random_state=42\n",")\n","print(\"Train class distribution:\", np.bincount(y_train))\n","print(\"Test  class distributino:\", np.bincount(y_test))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-04-29T03:26:17.416899Z","iopub.status.busy":"2025-04-29T03:26:17.416604Z","iopub.status.idle":"2025-04-29T03:26:21.605712Z","shell.execute_reply":"2025-04-29T03:26:21.604729Z","shell.execute_reply.started":"2025-04-29T03:26:17.416878Z"},"trusted":true},"outputs":[],"source":["result = binary_model.fit(\n","    X_train, y_train,\n","    validation_data=(X_test, y_test),\n","    epochs=10,\n","    batch_size=32\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-04-29T03:26:23.162089Z","iopub.status.busy":"2025-04-29T03:26:23.161751Z","iopub.status.idle":"2025-04-29T03:26:23.929572Z","shell.execute_reply":"2025-04-29T03:26:23.928637Z","shell.execute_reply.started":"2025-04-29T03:26:23.162067Z"},"trusted":true},"outputs":[],"source":["probs = binary_model.predict(X_test).ravel() \n","predicted = (probs > 0.5).astype(int) \n","\n","reverse_mapping = {1:\"Not Hollow\", 0:\"Hollow\"}\n","\n","n = 10\n","indexes = np.random.choice(len(X_test), size=n, replace=False)\n","\n","fig, axes = plt.subplots(2, 5, figsize=(15,6))\n","axes = axes.flatten()\n","\n","for ax, i in zip(axes, indexes):\n","    img  = X_test[i].squeeze()\n","    true_lbl = y_test[i]\n","    pred_lbl = predicted[i]  \n","\n","    ax.imshow(img, cmap='gray')\n","    ax.set_title(\n","        f\"True: {reverse_mapping[true_lbl]}\\n\"\n","        f\"Pred: {reverse_mapping[pred_lbl]}\"\n","    )\n","    ax.axis('off')\n","\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["cnn_model.save(\"binary_model.h5\")"]},{"cell_type":"markdown","metadata":{},"source":["### Predicting on images from filtering "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-04-29T04:46:02.521375Z","iopub.status.busy":"2025-04-29T04:46:02.520809Z","iopub.status.idle":"2025-04-29T04:46:02.584091Z","shell.execute_reply":"2025-04-29T04:46:02.583192Z","shell.execute_reply.started":"2025-04-29T04:46:02.521342Z"},"trusted":true},"outputs":[],"source":["import cv2\n","\n","def binarize_and_invert(img, thresh=127):\n","    _, binary = cv2.threshold(img, thresh, 255, cv2.THRESH_BINARY)\n","    inverted = 255 - binary\n","    return inverted\n","\n","# Apply to your DataFrame\n","df_images['binarized'] = df_images['image'].apply(binarize_and_invert)\n","\n","# Quick sanity check on shapes and dtypes\n","print(df_images[['id', 'binarized']].head())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-04-29T04:46:07.641352Z","iopub.status.busy":"2025-04-29T04:46:07.641040Z","iopub.status.idle":"2025-04-29T04:46:08.817490Z","shell.execute_reply":"2025-04-29T04:46:08.816237Z","shell.execute_reply.started":"2025-04-29T04:46:07.641332Z"},"trusted":true},"outputs":[],"source":["X = df_images[\"binarized\"].tolist()\n","X = np.stack(X) \n","X = X[..., np.newaxis].astype(\"float32\") / 255.0\n","\n","probs = binary_model.predict(X)\n","preds = (probs > 0.5).astype(int)\n","\n","preds_flat = preds.ravel() \n","id_to_pred = dict(zip(df_images[\"id\"], preds_flat))\n","id_to_pred = {int(k): int(v) for k, v in id_to_pred.items()}\n","\n","print(id_to_pred)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-04-29T04:46:18.485292Z","iopub.status.busy":"2025-04-29T04:46:18.484905Z","iopub.status.idle":"2025-04-29T04:46:20.345270Z","shell.execute_reply":"2025-04-29T04:46:20.343744Z","shell.execute_reply.started":"2025-04-29T04:46:18.485269Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","X = np.stack(df_images[\"binarized\"].tolist(), axis=0)\n","X = X[..., np.newaxis].astype(\"float32\") / 255.0\n","\n","probs = binary_model.predict(X).ravel()\n","preds = (probs > 0.5).astype(int)\n","\n","reverse_mapping = {0:\"Hollow\", 1:\"Not Hollow\"}\n","\n","n = 10\n","indexes = np.random.choice(len(X), size=n, replace=False)\n","\n","fig, axes = plt.subplots(2, 5, figsize=(15,6))\n","for ax, i in zip(axes.flatten(), indexes):\n","    img = X[i].squeeze()\n","    pred_lbl = preds[i]\n","    label_txt = reverse_mapping[pred_lbl]\n","\n","    ax.imshow(img, cmap='gray')\n","    ax.set_title(f\"{label_txt}\\n\")\n","    ax.axis('off')\n","\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["# Final Image"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-04-29T04:53:11.205801Z","iopub.status.busy":"2025-04-29T04:53:11.205478Z","iopub.status.idle":"2025-04-29T04:53:11.918606Z","shell.execute_reply":"2025-04-29T04:53:11.917419Z","shell.execute_reply.started":"2025-04-29T04:53:11.205779Z"},"trusted":true},"outputs":[],"source":["img, notes = ACCEPTED_NOTES [5]\n","img_pil = Image.fromarray(img)\n","\n","staff_objects = extract_staffs_from_image(img_pil)\n","\n","fig, ax = plt.subplots(figsize=(14, 8))\n","ax.imshow(gray_full, cmap='gray')\n","ax.set_title(\"Note Positions with Pitches\")\n","\n","for idx, (((cx, cy), (MA, ma), ang), num_id) in enumerate(notes):\n","    pred_lbl = reverse_mapping[id_to_pred[num_id]]\n","    \n","    pitch, step = calculate_pitch_multi(cy, staff_objects)\n","    # draw it\n","    circ = plt.Circle((cx, cy), radius=5, color='red', fill=True)\n","    ax.add_patch(circ)\n","\n","    ax.text(cx + 10, cy + 20, f\"{pitch}\",\n","            color='blue', fontsize=9, verticalalignment='center')\n","plt.axis('off')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-04-29T04:53:53.515415Z","iopub.status.busy":"2025-04-29T04:53:53.515017Z","iopub.status.idle":"2025-04-29T04:53:53.997916Z","shell.execute_reply":"2025-04-29T04:53:53.997001Z","shell.execute_reply.started":"2025-04-29T04:53:53.515392Z"},"trusted":true},"outputs":[],"source":["fig, ax = plt.subplots(figsize=(14, 8))\n","ax.imshow(gray_full, cmap='gray')\n","ax.set_title(\"Note Positions with Classification\")\n","for idx, (((cx, cy), (MA, ma), ang), num_id) in enumerate(notes):\n","    ax.text(cx + 10, cy + 20, f\"{id_to_pred[num_id]}\",\n","            color='blue', fontsize=9, verticalalignment='center')\n","\n","plt.axis('off')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Appendix\n","\n","Reference Github: https://github.com/BreezeWhite/oemer\n","\n","Importing sample image from the [DoReMi](https://github.com/steinbergmedia/DoReMi/?tab=readme-ov-file#OMR-metadata) dataset."]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":5465455,"sourceId":9062847,"sourceType":"datasetVersion"},{"datasetId":7280077,"sourceId":11606718,"sourceType":"datasetVersion"}],"dockerImageVersionId":31012,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"}},"nbformat":4,"nbformat_minor":4}
