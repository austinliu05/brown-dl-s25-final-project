{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9062847,"sourceType":"datasetVersion","datasetId":5465455},{"sourceId":11619735,"sourceType":"datasetVersion","datasetId":7280077}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Defining Classes","metadata":{}},{"cell_type":"code","source":"doremi_dir = \"/kaggle/input/doremi-raw-dataset/DoReMi_v1/\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T20:51:04.002876Z","iopub.execute_input":"2025-05-02T20:51:04.004002Z","iopub.status.idle":"2025-05-02T20:51:04.008484Z","shell.execute_reply.started":"2025-05-02T20:51:04.003966Z","shell.execute_reply":"2025-05-02T20:51:04.007254Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Staff:\n    def __init__(self, center: float, upper_bound: int, lower_bound: int, unit_size: float, track: int):\n        self.center = center\n        self.upper_bound = upper_bound\n        self.lower_bound = lower_bound\n        self.unit_size = unit_size\n        self.track = track\n\n    def __repr__(self):\n        return (\n            \"Staff {\\n\"\n            f\"    Center: {self.center:.1f}\\n\"\n            f\"    Upper bound: {self.upper_bound}\\n\"\n            f\"    Lower bound: {self.lower_bound}\\n\"\n            f\"    Unit size: {self.unit_size:.1f}\\n\"\n            f\"    Track: {self.track}\\n\"\n            \"}\"\n        )\n        \nclass Note:\n    def __init__(self, class_name: str, top: int, left: int, width: int, height: int, img_name: str):\n        self.class_name = class_name\n        self.top = top\n        self.left = left\n        self.width = width\n        self.height = height\n        self.image = img_name\n    def gather_coordinates(self):\n        return [self.top, self.left, self.width, self.height]\n    def create_box(self):\n        return [(self.left, self.top), (self.left + self.width, self.top + self.height)]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T20:51:04.811209Z","iopub.execute_input":"2025-05-02T20:51:04.812136Z","iopub.status.idle":"2025-05-02T20:51:04.822644Z","shell.execute_reply.started":"2025-05-02T20:51:04.812100Z","shell.execute_reply":"2025-05-02T20:51:04.821740Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Preprocessing Images","metadata":{}},{"cell_type":"markdown","source":"## Sample Image","metadata":{}},{"cell_type":"code","source":"from PIL import Image\n\nimg_path = doremi_dir + 'Images/Delius - String Quartet mvt III-003.png'\nimage = Image.open(img_path)\nfile_path = doremi_dir + 'Parsed_by_page_omr_xml/Parsed_Alkan - Posement-layout-0-muscima_Page_1.xml'\nwith open(file_path, 'r') as file:\n    content = file.read(200)\n    print(content)\nimage","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T20:51:07.264554Z","iopub.execute_input":"2025-05-02T20:51:07.265236Z","iopub.status.idle":"2025-05-02T20:51:07.309678Z","shell.execute_reply.started":"2025-05-02T20:51:07.265201Z","shell.execute_reply":"2025-05-02T20:51:07.308829Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Preprocessing Stafflines","metadata":{}},{"cell_type":"code","source":"import re\nimport pprint\nimport os\n\ndef natural_key(string):\n    return [int(s) if s.isdigit() else s.lower() for s in re.split('(\\d+)', string)]\n\n# Sorts Images and Parsed_by_page_omr_xml directories\nimage_list = os.listdir(doremi_dir + \"Images\")\nomr_xml_list =  os.listdir(doremi_dir + \"Parsed_by_page_omr_xml\")\n\nimage_list.sort(key=natural_key)\nomr_xml_list.sort(key=natural_key)\n\n# Matches each .png file to their respective omr_xml file\nimage_to_omr = {img: omr for img, omr in zip(image_list, omr_xml_list)}\n\ndef extract_staffs_from_image(cropped_image, padding_multiplier=4.5):\n    image_gray = cropped_image.convert(\"L\")\n    image_np = np.array(image_gray)\n    _, binary = cv2.threshold(image_np, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n    horizontal_projection = np.sum(binary, axis=1)\n    peaks, _ = find_peaks(horizontal_projection, height=np.max(horizontal_projection) * 0.5, distance=5)\n\n    grouped_staffs = []\n    for i in range(0, len(peaks) - 4, 5):\n        group = peaks[i:i+5]\n        grouped_staffs.append(group)\n\n    staff_objects = []\n    for i, group in enumerate(grouped_staffs):\n        y_center = float(np.mean(group))\n        unit_size = float(np.mean(np.diff(group)))\n        padding = int(unit_size * padding_multiplier)\n\n        y_min = int(np.min(group) - padding)\n        y_max = int(np.max(group) + padding)\n        track = i % 2 + 1\n\n        staff = Staff(\n            center=y_center,\n            upper_bound=y_min,\n            lower_bound=y_max,\n            unit_size=unit_size,\n            track=track\n        )\n        staff_objects.append(staff)\n\n    return staff_objects","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T20:51:09.949615Z","iopub.execute_input":"2025-05-02T20:51:09.950266Z","iopub.status.idle":"2025-05-02T20:51:10.023289Z","shell.execute_reply.started":"2025-05-02T20:51:09.950241Z","shell.execute_reply":"2025-05-02T20:51:10.022591Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from PIL import Image\nimport os\nimport numpy as np\nimport cv2 \nfrom scipy.signal import find_peaks\n\nimages = []\nstaff_data_per_image = {}\n\nselected_files = [\n    \"Delius - String Quartet mvt III\",\n    \"Schumann - String Quartet 1 mvt 3\",\n    \"Webern - Variations Op 27 III\"]\n\nfor image_name in image_list:\n    if not any(image_name.startswith(prefix) for prefix in selected_files):\n        continue\n\n    image_path = os.path.join(doremi_dir, \"Images\", image_name)\n    try:\n        cropped_image = Image.open(image_path)\n        staff_objects = extract_staffs_from_image(cropped_image)\n\n        images.append(image_name)\n        staff_data_per_image[image_name] = staff_objects \n\n        if len(images) == 200:\n            break\n    except Exception as e:\n        print(f\"Skipping {image_name} due to error: {e}\")\n        continue\n\n# Confirm results\nprint(f\"\\nFound {len(images)} images.\\n\") \nprint(images)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T20:51:11.455031Z","iopub.execute_input":"2025-05-02T20:51:11.455580Z","iopub.status.idle":"2025-05-02T20:51:16.806101Z","shell.execute_reply.started":"2025-05-02T20:51:11.455555Z","shell.execute_reply":"2025-05-02T20:51:16.805126Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"max_height = 0\nmax_image_name = None\nmax_upper = None\nmax_lower = None\n\nIMG_TO_STAFF = {}\n\nfor image_name in images:\n    image_path = os.path.join(doremi_dir + \"Images\", image_name)\n    img = Image.open(image_path)\n\n    staff_objects = extract_staffs_from_image(img)\n    n = len(staff_objects)\n    if max_upper == None or staff_objects[0].upper_bound < max_upper:\n        max_upper = staff_objects[0].upper_bound\n        \n    if max_lower == None or staff_objects[n - 1].lower_bound > max_lower:\n        max_lower = staff_objects[n - 1].lower_bound\n\n    IMG_TO_STAFF[image_name] = staff_objects\n\nprint(f\"Upper bound: {max_upper}\")\nprint(f\"Lower bound: {max_lower}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T20:51:19.258313Z","iopub.execute_input":"2025-05-02T20:51:19.258976Z","iopub.status.idle":"2025-05-02T20:51:24.511556Z","shell.execute_reply.started":"2025-05-02T20:51:19.258950Z","shell.execute_reply":"2025-05-02T20:51:24.510675Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nfrom scipy.signal import find_peaks\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimage_gray = image.convert(\"L\")\nimage_np = np.array(image_gray)\n\n_, binary = cv2.threshold(image_np, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n\nhorizontal_projection = np.sum(binary, axis=1)\n\npeaks, _ = find_peaks(horizontal_projection, height=np.max(horizontal_projection) * 0.5, distance=5)\n\ngrouped_staffs = []\nfor i in range(0, len(peaks) - 4, 5):\n    group = peaks[i:i+5]\n    grouped_staffs.append(group)\n\nimage_color = np.stack([image_np]*3, axis=-1)\n\nstaff_bounds = []\nstaff_bounds = []\nstaff_objects = []\n\nfor i, group in enumerate(grouped_staffs):\n    y_center = float(np.mean(group))\n    unit_size = float(np.mean(np.diff(group)))\n    padding = int(unit_size * 3.5)\n\n    y_min = int(np.min(group) - padding)\n    y_max = int(np.max(group) + padding)\n    track = i % 2 + 1\n\n    staff = Staff(\n        center=y_center,\n        upper_bound=y_min,\n        lower_bound=y_max,\n        unit_size=unit_size,\n        track=track\n    )\n    staff_objects.append(staff)\n    staff_bounds.append((y_min, y_max))\n\n    cv2.rectangle(image_color, (0, y_min), (image_np.shape[1], y_max), (0, 255, 0), 2)\n    print(staff)\n\n\nplt.figure(figsize=(20, 20))\nplt.imshow(image_color)\nplt.axis(\"off\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T20:51:26.559434Z","iopub.execute_input":"2025-05-02T20:51:26.559962Z","iopub.status.idle":"2025-05-02T20:51:27.836030Z","shell.execute_reply.started":"2025-05-02T20:51:26.559936Z","shell.execute_reply":"2025-05-02T20:51:27.835179Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Preprocessing Notes","metadata":{}},{"cell_type":"code","source":"import xml.etree.ElementTree as ET\n\nIMAGE_TO_NOTES = {}\nfor key in images:\n# Parse the XML file\n    xml_path = os.path.join(doremi_dir, \"Parsed_by_page_omr_xml\", image_to_omr[key])\n    tree = ET.parse(xml_path) \n    root = tree.getroot()\n# Define target classnames\n    target_classes = {'noteheadBlack', 'noteheadHalf', 'noteheadWhole'}\n\n# Collect matching nodes\n    matching_nodes = []\n\n# Loop through all <Node> elements\n    for node in root.findall('.//Node'):\n        class_name = node.find('ClassName')\n        if class_name is not None and class_name.text in target_classes:\n            matching_nodes.append(node)\n\n# Print results\n    final_notes = []\n    for match in matching_nodes:\n        note = Note(match.find(\"ClassName\").text, \n                    int(match.find(\"Top\").text), \n                    int(match.find(\"Left\").text), \n                    int(match.find(\"Width\").text), \n                    int(match.find(\"Height\").text), \n                    key\n                   )\n        final_notes.append(note)\n        # print(note.image)\n    IMAGE_TO_NOTES[key] = final_notes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T20:51:30.054928Z","iopub.execute_input":"2025-05-02T20:51:30.055240Z","iopub.status.idle":"2025-05-02T20:51:30.309269Z","shell.execute_reply.started":"2025-05-02T20:51:30.055217Z","shell.execute_reply":"2025-05-02T20:51:30.308344Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Cropping Stafflines","metadata":{}},{"cell_type":"code","source":"CROPPED_NOTE_PAIRS = []\nfor image_name, notes in IMAGE_TO_NOTES.items():\n    image_path = os.path.join(doremi_dir + \"Images\", image_name)\n    img = Image.open(image_path)\n    width = img.width\n    \n    # Crop image to vertical slice (top to bottom)\n    cropped = img.crop((0, max_upper, width, max_lower))\n    for note in notes:\n        note.top -= max_upper\n    CROPPED_NOTE_PAIRS.append((cropped, notes, image_name))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T20:51:32.550339Z","iopub.execute_input":"2025-05-02T20:51:32.550954Z","iopub.status.idle":"2025-05-02T20:51:33.511541Z","shell.execute_reply.started":"2025-05-02T20:51:32.550929Z","shell.execute_reply":"2025-05-02T20:51:33.510879Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from PIL import Image, ImageDraw\n\nfor i in range(5):\n    test_image, notes, _ = CROPPED_NOTE_PAIRS[i]\n    img = test_image.copy()  \n    draw = ImageDraw.Draw(img)\n\n    # Draw bounding boxes on the image\n    counter = 0;\n    for note in notes:\n        draw.rectangle(note.create_box(), outline=\"red\", width=2)\n        draw.text(note.create_box()[1], str(counter), fill=\"black\")\n        counter += 1\n    display(img)\n    width, height = img.size\n    print(width)\n    print(height)\n        \ntest_image, notes, _ = CROPPED_NOTE_PAIRS[0]\nimg = test_image.copy()  \nsample_cropped_img = test_image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T20:52:23.125842Z","iopub.execute_input":"2025-05-02T20:52:23.126168Z","iopub.status.idle":"2025-05-02T20:52:23.232576Z","shell.execute_reply.started":"2025-05-02T20:52:23.126142Z","shell.execute_reply":"2025-05-02T20:52:23.231736Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Extracting Note","metadata":{}},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt \n\ndef remove_staff_lines(binary_img: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Detect and remove horizontal staff lines from the binary image input. \n\n    Output: \n        np.ndarray: a binary image with horizontal staff lines removed.\n    \"\"\"\n    horizontal_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (40,1))\n    detected_stafflines = cv2.morphologyEx(binary_img, cv2.MORPH_OPEN, horizontal_kernel, iterations=1)\n    return cv2.subtract(binary_img, detected_stafflines)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T20:52:27.290665Z","iopub.execute_input":"2025-05-02T20:52:27.290996Z","iopub.status.idle":"2025-05-02T20:52:27.296425Z","shell.execute_reply.started":"2025-05-02T20:52:27.290975Z","shell.execute_reply":"2025-05-02T20:52:27.295527Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport numpy as np\n\ndef selective_hole_fill(binary, min_hole_area, max_hole_area) -> np.ndarray:\n    \"\"\"\n    Fill only fully enclosed holes whose area is between min_hole_area and max_hole_area.\n    \"\"\"\n    # invert the binary image\n    inv = cv2.bitwise_not(binary)\n    \n    # label connected components in inverted image\n    num, labels, stats, centroids = cv2.connectedComponentsWithStats(inv, connectivity=8)\n    \n    h, w = binary.shape\n    filled = binary.copy()\n\n    # filling step\n    for lbl in range(1, num): # iterated thru each component\n        area = stats[lbl, cv2.CC_STAT_AREA] \n        # only fill holes in the desired size range\n        if area < min_hole_area or area > max_hole_area:\n            continue\n        filled[labels == lbl] = 255\n\n    return filled","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T20:52:28.528389Z","iopub.execute_input":"2025-05-02T20:52:28.529233Z","iopub.status.idle":"2025-05-02T20:52:28.534800Z","shell.execute_reply.started":"2025-05-02T20:52:28.529204Z","shell.execute_reply":"2025-05-02T20:52:28.533894Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"WIDTH_FACTOR = 0.5\nHEIGHT_FACTOR = 0.4\nMIN_AREA = 100 # pixels\nMAX_AREA = 600\n\nMAX_HEIGHT_WIDTH_RATIO = 2.0 # h / w\nMIN_WIDTH_HEIGHT_RATIO = 1 # w / h \nELLIPSE_SIZE = 20.5\nPAD = 4\n\ndef extract_noteheads(img, debug: bool) -> np.ndarray:\n    # testing with 'cropped_img' variable\n    gray_img = img.convert('L')\n    gray_img = np.array(gray_img, dtype=np.uint8)\n\n    # assign all pixel values higher than 127 to 255 and others to 0\n    _, binary_img = cv2.threshold(gray_img, 127, 255, cv2.THRESH_BINARY_INV)\n    \n    if debug:\n        plt.figure(figsize=(6,6))\n        plt.imshow(binary_img, cmap='gray')\n        plt.title(\"Binary Inverted\")\n        plt.axis('off')\n        plt.show()\n        \n    # remove staff lines\n    processed_img = remove_staff_lines(binary_img)\n\n    # Performing morphological operations\n    w_close = max(1, int(round(ELLIPSE_SIZE * 0.2)))\n    h_close = max(1, int(round(ELLIPSE_SIZE * 0.4)))\n    close_krn = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (w_close, h_close))\n    closed = cv2.morphologyEx(processed_img, cv2.MORPH_CLOSE, close_krn)\n    \n    filled = selective_hole_fill(\n        closed, \n        min_hole_area=150,\n        max_hole_area=250,\n        )\n    \n    if debug:\n        plt.figure(figsize=(6,6))\n        plt.imshow(closed, cmap='gray')\n        plt.title(\"After Closed\")\n        plt.axis('off')\n        plt.show()\n           \n    notehead_size = (\n        int(round(ELLIPSE_SIZE * WIDTH_FACTOR)),\n        int(round(ELLIPSE_SIZE * HEIGHT_FACTOR))\n    )\n    \n    # erode away isolated pixels that are not note heads\n    kernel_2 = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, notehead_size)\n    opened = cv2.morphologyEx(filled, cv2.MORPH_OPEN, kernel_2)\n\n    if debug:\n        plt.figure(figsize=(6,6))\n        plt.imshow(opened, cmap='gray')\n        plt.title(\"After Opened\")\n        plt.axis('off')\n        plt.show()\n        \n    w_close = max(1, int(round(ELLIPSE_SIZE * 0.1)))\n    h_close = max(1, int(round(ELLIPSE_SIZE * 0.8)))\n    close_krn = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (w_close, h_close))\n    closed = cv2.morphologyEx(opened, cv2.MORPH_CLOSE, close_krn)\n    \n    if debug:\n        plt.figure(figsize=(6,6))\n        plt.imshow(closed, cmap='gray')\n        plt.title(\"After Second Closed\")\n        plt.axis('off')\n        plt.show()\n        \n    # find contours\n    contours, _ = cv2.findContours(closed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    \n    if debug:\n        canvas = cv2.cvtColor(closed, cv2.COLOR_GRAY2BGR)\n        \n        cv2.drawContours(canvas, contours, -1, (0,255,0), 1)\n        \n        plt.figure(figsize=(6,6))\n        plt.imshow(canvas[...,::-1])\n        plt.title(f'{len(contours)} contours found')\n        plt.axis('off')\n        plt.show()\n\n    # filter out shapes that are not noteheads by checking how round the white area is\n    notehead_mask = np.zeros_like(opened)\n    for item in contours:\n        area = cv2.contourArea(item)\n        # print(area)\n        if area < MIN_AREA or area > MAX_AREA:\n            continue\n        \n        x, y, w, h = cv2.boundingRect(item)\n        \n        # Drop any blob that's too tall/thin:\n        if h / float(w) > MAX_HEIGHT_WIDTH_RATIO:\n            continue\n\n        # Drop any blob that's too wide/flat:\n        if w / float(h) < MIN_WIDTH_HEIGHT_RATIO:\n            continue\n\n        cv2.drawContours(notehead_mask, [item], -1, 255, -1)\n\n    morphed = notehead_mask\n\n    if debug:\n        plt.figure(figsize=(6,6))\n        plt.imshow(morphed, cmap='gray')\n        plt.title(\"Morphed (staff lines out, noteheads closed)\")\n        plt.axis('off')\n        plt.show()\n        \n    final_contours, _ = cv2.findContours(\n        notehead_mask,\n        cv2.RETR_EXTERNAL,\n        cv2.CHAIN_APPROX_SIMPLE\n    )\n\n    return morphed, final_contours","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T20:52:30.212368Z","iopub.execute_input":"2025-05-02T20:52:30.212923Z","iopub.status.idle":"2025-05-02T20:52:30.228251Z","shell.execute_reply.started":"2025-05-02T20:52:30.212893Z","shell.execute_reply":"2025-05-02T20:52:30.227360Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_image, notes, _ = CROPPED_NOTE_PAIRS[5]\nmorphed_img, contours = extract_noteheads(test_image, True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T20:52:37.929320Z","iopub.execute_input":"2025-05-02T20:52:37.929615Z","iopub.status.idle":"2025-05-02T20:52:40.020346Z","shell.execute_reply.started":"2025-05-02T20:52:37.929594Z","shell.execute_reply":"2025-05-02T20:52:40.019675Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def display_image(image, title=\"Image\"):\n    plt.figure(figsize=(10, 8))\n    plt.imshow(image, cmap='gray')\n    plt.title(title)\n    plt.axis('off')\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T20:52:41.864121Z","iopub.execute_input":"2025-05-02T20:52:41.864414Z","iopub.status.idle":"2025-05-02T20:52:41.869373Z","shell.execute_reply.started":"2025-05-02T20:52:41.864394Z","shell.execute_reply":"2025-05-02T20:52:41.868549Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_roi_coords(mask):\n    \"\"\"\n    Compute padded bounding-box coordinates for a binary mask.\n    Returns (x0, y0, x1, y1).\n    \"\"\"\n    x, y, w, h = cv2.boundingRect(mask)\n    # make sure we don't go out of bounds\n    x0 = max(0, x - PAD) \n    y0 = max(0, y - PAD)\n    x1 = min(mask.shape[1], x + w + PAD)\n    y1 = min(mask.shape[0], y + h + PAD)\n    return x0, y0, x1, y1\n\ndef split_horizontal_blob(mask, debug=False):\n    \"\"\"\n    Split a single-blob mask horizontally through its vertical midpoint.\n    Returns [top_half, bottom_half] or [mask] if one half is empty.\n    \"\"\"\n    # Compute bounding box\n    x, y, w, h = cv2.boundingRect(mask)\n    mid_y = y + h // 2 # calculating the midway point\n\n    # Empty masks\n    top = np.zeros_like(mask)\n    bottom = np.zeros_like(mask)\n\n    # Slicing within bbox\n    top[y:mid_y, x:x+w] = mask[y:mid_y, x:x+w]\n    bottom[mid_y:y+h, x:x+w] = mask[mid_y:y+h, x:x+w]\n    pieces = [top, bottom]\n\n    # If one half is empty, return original. This means that either mask is completely all black\n    if cv2.countNonZero(top) == 0 or cv2.countNonZero(bottom) == 0:\n        return [mask]\n\n    # Creating a kernel\n    radius = 5\n    ksize = 2 * radius + 1\n    kern  = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (ksize, ksize))\n\n    # apply opening to each piece to erode straight edges, then dilate back\n    # basically erosding away the straight edges and then dilating again to give it its shape back\n    rounded = [cv2.morphologyEx(p, cv2.MORPH_OPEN, kern) for p in pieces]\n\n    if debug:\n        for i, m in enumerate(rounded):\n            x0, y0, x1, y1 = get_roi_coords(mask)\n            plt.figure(figsize=(3,3))\n            plt.imshow(m[y0:y1, x0:x1], cmap='gray')\n            plt.title(f'Rounded Piece #{i}')\n            plt.axis('off')\n        plt.show()\n\n    return rounded","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T20:52:43.463354Z","iopub.execute_input":"2025-05-02T20:52:43.463637Z","iopub.status.idle":"2025-05-02T20:52:43.474539Z","shell.execute_reply.started":"2025-05-02T20:52:43.463617Z","shell.execute_reply":"2025-05-02T20:52:43.473634Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def split_blob(mask, debug=False):\n    \"\"\"\n    Split a large binary mask into smaller ones using watershed,until each piece's nonzero area <= MAX_AREA.\n    Returns a list of mask pieces.\n    \"\"\"\n    # If this mask is already small enough, return it. Error checking bascially\n    area = cv2.countNonZero(mask)\n    if area <= MAX_AREA:\n        if debug:\n            x0, y0, x1, y1 = get_roi_coords(mask)\n            plt.figure(figsize=(4,4))\n            plt.imshow(mask[y0:y1, x0:x1], cmap='gray', vmin=0, vmax=255, interpolation='nearest')\n            plt.title(f'Final Piece (area={area})')\n            plt.axis('off')\n        return [mask]\n\n    # 1) Compute distance transform\n    # Computes shortest distance from foreground pixel to the background pixel. Identifying the center regions of the object\n    dist = cv2.distanceTransform(mask, cv2.DIST_L2, 5)\n    if debug:\n        x0, y0, x1, y1 = get_roi_coords(mask)\n\n        plt.figure(figsize=(4,4))\n        plt.imshow(dist[y0:y1, x0:x1], cmap='viridis', interpolation='nearest')\n        plt.title('Distance Transform')\n        plt.axis('off')\n\n    # 2) Get sure foreground peaks (areas that are confidently park of the foreground, deep enough within the object)\n    # Extracting deepest pixels from background boundary, 40% of blob distance\n    thresh_val = 0.6 * dist.max()\n    _, fg = cv2.threshold(dist, thresh_val, 255, cv2.THRESH_BINARY)\n    fg = fg.astype(np.uint8) # converting to be used in OpenCV\n    if debug:\n        x0, y0, x1, y1 = get_roi_coords(mask)\n\n        plt.figure(figsize=(4,4))\n        plt.imshow(fg[y0:y1, x0:x1], cmap='gray', vmin=0, vmax=255, interpolation='nearest')\n        plt.title('Sure Foreground Peaks')\n        plt.axis('off')\n\n    # 3) Create markers for watershed\n    num_components, markers = cv2.connectedComponents(fg)\n    if num_components < 2: # only one component in the foreground\n        return [mask]\n    markers = markers + 1 # 0 is reserved for the background\n    unknown = cv2.subtract(mask, fg) # marking the unknown region\n    markers[unknown == 255] = 0 # uncertain pixels\n    \n    if debug:\n        x0, y0, x1, y1 = get_roi_coords(mask)\n\n        plt.figure(figsize=(4,4))\n        plt.imshow(markers[y0:y1, x0:x1], cmap='tab20', interpolation='nearest')\n        plt.title('Watershed Markers')\n        plt.axis('off')\n\n    # 4) Apply watershed\n    color = cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR) # watershed needs a colored image\n    cv2.watershed(color, markers) # running watershed, flooding from seed labels\n    \n    if debug:\n        boundary_overlay = color.copy()\n        boundary_overlay[markers == -1] = (0,0,255)\n        x0, y0, x1, y1 = get_roi_coords(mask)\n\n        plt.figure(figsize=(4,4))\n        plt.imshow(boundary_overlay[y0:y1, x0:x1][..., ::-1], interpolation='nearest')\n        plt.title('Watershed Boundaries ')\n        plt.axis('off')\n\n    # 5) Extract pieces\n    pieces = []\n    for lbl in range(2, num_components+1):\n        piece = np.zeros_like(mask)\n        piece[markers == lbl] = 255\n        if cv2.countNonZero(piece) > 0:\n            if debug:\n                x0, y0, x1, y1 = get_roi_coords(piece)\n                plt.figure(figsize=(4,4))\n                plt.imshow(piece[y0:y1, x0:x1], cmap='gray', vmin=0, vmax=255, interpolation='nearest')\n                plt.title(f'Piece {lbl-1}')\n                plt.axis('off')\n            pieces.append(piece)\n\n    # If watershed didn't split, return original\n    if not pieces:\n        return [mask]\n        \n    aspect_thresh = 1.2 # height has to be 120% of width, if its greater, that means there are two notes on top of each other\n    pieces = []\n    \n    for lbl in range(2, num_components+1):\n        piece = np.zeros_like(mask)\n        piece[markers == lbl] = 255\n        if cv2.countNonZero(piece) > 0:\n            # checking aspect ratio\n            x,y,w,h = cv2.boundingRect(piece)\n                \n            if h > w * aspect_thresh or h * aspect_thresh > w:\n                if True:\n                    x0, y0, x1, y1 = get_roi_coords(piece)\n                    plt.figure(figsize=(4,4))\n                    plt.imshow(piece[y0:y1, x0:x1], cmap='gray', vmin=0, vmax=255, interpolation='nearest')\n                    plt.title(f'Piece {lbl-1} height: {h}, width: {w}')\n                    plt.axis('off')\n                # split horizontally if its too tall or too wide\n                sub = split_horizontal_blob(piece, True)\n                pieces.extend(sub)\n            else:\n                if debug:\n                    x0, y0, x1, y1 = get_roi_coords(piece)\n                    plt.figure(figsize=(4,4))\n                    plt.imshow(piece[y0:y1, x0:x1], cmap='gray', vmin=0, vmax=255, interpolation='nearest')\n                    plt.title(f'Piece {lbl-1} (zoomed)'); plt.axis('off')\n                pieces.append(piece)\n    return pieces","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T20:52:50.551835Z","iopub.execute_input":"2025-05-02T20:52:50.552463Z","iopub.status.idle":"2025-05-02T20:52:50.570828Z","shell.execute_reply.started":"2025-05-02T20:52:50.552431Z","shell.execute_reply":"2025-05-02T20:52:50.570050Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport numpy as np\nid_num = 0\ndef refine_and_fit_ellipses(gray_img, notehead_mask, contours,\n                            iou_thresh=0.75,\n                            min_area=150,\n                            min_ellipse_area=300,\n                            min_minor_axis=10,\n                            min_axis_ratio=0.5,\n                            min_area_ratio=0.6,\n                            MAX_AREA_ratio=1.3,\n                            min_solidity=0.9,\n                            debug=False,):\n    \"\"\"\n    Returns two lists:\n     - accepted: [( (cx,cy), (MA,ma), angle ), ... ]\n     - rejected: [ ( (cx,cy), reason_string ), ... ]\n    \"\"\"\n    H, W = gray_img.shape\n    accepted, rejected = [], []\n    global id_num\n    for cnt in contours:\n        # Computing the raw area\n        rawA = cv2.contourArea(cnt)\n        blob = np.zeros_like(notehead_mask)\n        cv2.drawContours(blob, [cnt], -1, 255, -1)\n        \n        if debug:\n            x0, y0, x1, y1 = get_roi_coords(blob)\n            \n            plt.figure(figsize=(4,4))\n            plt.imshow(blob[y0:y1, x0:x1],\n                       cmap='gray',\n                       vmin=0, vmax=255,\n                       interpolation='nearest')\n            plt.title(\"Blob mask\")\n            plt.axis('off')\n\n        # If raw area is bigger than the max area, split into multple blobs\n        if rawA > MAX_AREA:\n            submasks = split_blob(blob, False)\n        else:\n            submasks = [blob]\n\n        for piece in submasks:\n            # extracting pieces from the submask and adding PADding \n            x0, y0, x1, y1 = get_roi_coords(piece)\n\n            roi_gray = gray_img[y0:y1, x0:x1]\n            roi_mask = piece[y0:y1, x0:x1]\n            \n            if debug:\n                plt.figure(figsize=(4,4))\n                plt.imshow(roi_gray,\n                           cmap='gray',\n                           vmin=0, vmax=255,\n                           interpolation='nearest')\n                plt.title(\"ROI gray\")\n                plt.axis('off')\n                \n                plt.figure(figsize=(4,4))\n                plt.imshow(roi_mask,\n                           cmap='gray',\n                           vmin=0, vmax=255,\n                           interpolation='nearest')\n                plt.title(\"ROI mask\")\n                plt.axis('off')\n                \n            # finding the local contours within the mask\n            local_cnts, _ = cv2.findContours(\n            roi_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n            )\n\n            # Run 9 total tests\n            for lc in local_cnts:\n                area = cv2.contourArea(lc)\n                cx_loc, cy_loc = cv2.boundingRect(lc)[:2]\n                (cx_loc, cy_loc), (MA, ma), ang = cv2.fitEllipse(lc)\n                note_metadata = ((cx_loc+x0, cy_loc+y0),(MA,ma),ang)\n                \n                # 1) raw-area floor\n                if area < min_area or area > MAX_AREA:\n                    rejected.append(note_metadata)\n                    continue\n\n                # 2) need at least 5 points to fit an ellipse\n                if len(lc) < 5:\n                    rejected.append(note_metadata)\n                    continue\n\n                # 4) ellipse-area floor\n                ell_area = np.pi*(MA/2)*(ma/2)\n                if ell_area < min_ellipse_area:\n                    rejected.append(note_metadata)\n                    continue\n\n                # 5) minor-axis floor\n                if min(MA, ma) < min_minor_axis:\n                    rejected.append(note_metadata)\n                    continue\n\n                # 6) axis-ratio\n                axis_r = min(MA,ma)/max(MA,ma)\n                if axis_r < min_axis_ratio:\n                    rejected.append(note_metadata)\n                    continue\n\n                # 7) area-ratio\n                area_r = area/ell_area\n                if not (min_area_ratio <= area_r <= MAX_AREA_ratio):\n                    rejected.append(note_metadata)\n                    continue\n\n                # if we get here, it’s a keeper\n                accepted.append((note_metadata, id_num))\n                id_num+=1\n\n    return accepted, rejected","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T20:52:55.525401Z","iopub.execute_input":"2025-05-02T20:52:55.525700Z","iopub.status.idle":"2025-05-02T20:52:55.540347Z","shell.execute_reply.started":"2025-05-02T20:52:55.525678Z","shell.execute_reply":"2025-05-02T20:52:55.539523Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm import tqdm\ndebug = False\nACCEPTED_NOTES = []\nfor test_img, notes, image_n in tqdm(CROPPED_NOTE_PAIRS, desc=\"Extracting noteheads\", unit=\"pair\"):    \n    # 1) Make image gray & binary\n    if not isinstance(test_img, np.ndarray):\n        gray = np.array(test_img.convert('L'), dtype=np.uint8)\n    else:\n        gray = test_img.astype(np.uint8)\n    \n    _, binary = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY_INV)\n    \n    # 2) Extract noteheads\n    morphed, contours = extract_noteheads(\n        test_img,\n        debug=False\n    )\n    \n    # 3) Visualize the cleaned contours\n    if debug:\n        canvas = cv2.cvtColor(binary, cv2.COLOR_GRAY2BGR)\n        cv2.drawContours(canvas, contours, -1, (0,255,0), 2)\n        plt.figure(figsize=(6,6))\n        plt.imshow(canvas[..., ::-1])\n        plt.title(f\"Cleaned Image {i}: {len(contours)} contours\")\n        plt.axis('off')\n        plt.show()\n        \n    # 4) Now call the new fitter\n    accepted, rejected = refine_and_fit_ellipses(\n        gray_img = gray,\n        notehead_mask = morphed,\n        contours = contours,\n        min_area = MIN_AREA,\n        debug = False\n    )\n    ACCEPTED_NOTES.append((gray, accepted, image_n))\n    if debug:\n        print(f\"Accepted: {len(accepted)} ellipses\")\n        print(f\"Rejected: {len(rejected)} blobs\")\n        \n        # 5) Overlay the kept ellipses on your original\n        canvas2 = cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR)\n        for (cx,cy), (MA,ma), ang in accepted:\n            cv2.ellipse(\n                canvas2,\n                (int(round(cx)), int(round(cy))),\n                (int(round(MA/2)), int(round(ma/2))),\n                angle=ang,\n                startAngle=0,\n                endAngle=360,\n                color=(0,0,255),\n                thickness=2\n            )\n        \n        plt.figure(figsize=(8,8), dpi=200)\n        plt.imshow(canvas2[..., ::-1])\n        plt.title(f\"Accepted Ellipses: {len(accepted)}\")\n        plt.axis('off')\n        plt.show()\n         \n        canvas2 = cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR)\n        for (cx,cy), (MA,ma), ang in rejected:\n            cv2.ellipse(\n                canvas2,\n                (int(round(cx)), int(round(cy))),\n                (int(round(MA/2)), int(round(ma/2))),\n                angle=ang,\n                startAngle=0,\n                endAngle=360,\n                color=(0,0,255),\n                thickness=2\n            )\n            \n        plt.figure(figsize=(8,8), dpi=200)\n        plt.imshow(canvas2[..., ::-1])\n        plt.title(f\"Rejected Ellipses: {len(rejected)}\")\n        plt.axis('off')\n        plt.show()\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T20:55:59.962237Z","iopub.execute_input":"2025-05-02T20:55:59.963205Z","iopub.status.idle":"2025-05-02T20:56:11.583053Z","shell.execute_reply.started":"2025-05-02T20:55:59.963169Z","shell.execute_reply":"2025-05-02T20:56:11.582262Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"debug = False\nTARGET_SIZE = (39, 23)\n\ndef crop_pad_notes(gray_img, accepted, debug = False):\n    H, W = gray_img.shape[:2]\n    bboxes = []\n    for ((cx, cy), (MA, ma), _), id_num in accepted:\n        # center X, center Y\n        # Major Axis - width\n        # Minor axis - height\n        w = int(round(MA))\n        h = int(round(ma))\n        x0 = max(0, int(round(cx - w/2)))\n        y0 = max(0, int(round(cy - h/2)))\n        x1 = min(W, int(round(cx + w/2)))\n        y1 = min(H, int(round(cy + h/2)))\n        bboxes.append(((x0, y0, x1, y1), id_num))\n\n    # finding universal size\n    IMAGE_WIDTH, IMAGE_HEIGHT = 0,0\n    for (x0,y0,x1,y1), _ in bboxes:\n        IMAGE_WIDTH = max(x1 - x0, IMAGE_WIDTH)\n        IMAGE_HEIGHT = max(y1 - y0, IMAGE_HEIGHT)\n\n    if debug:\n        print(f\"IMAGE_WIDTH: {IMAGE_WIDTH} IMAGE_HEIGHT: {IMAGE_HEIGHT}\")\n    \n    padded_imgs = []\n    for (x0,y0,x1,y1), id_num in bboxes:\n        cropped = gray_img[y0:y1, x0:x1]\n        # print(id_num)\n        padded_imgs.append((cropped, (x0,y0,x1,y1), id_num))\n    return padded_imgs\n\nIMAGE_DATA = []\nfor img, accepted, image_name in tqdm(ACCEPTED_NOTES, desc=\"Cropping noteheads\", unit=\"pair\"):\n    img_arr = np.array(img)\n    cropped_images = crop_pad_notes(img_arr, accepted)\n    for idx, (crop_img, bbox, id_num) in enumerate(cropped_images):\n        if debug:\n            plt.figure(figsize=(3,3))\n            plt.imshow(crop_img, cmap='gray', vmin=0, vmax=255)\n            plt.title(f\"Cropped Note {idx+1}\")\n            plt.axis('off')\n            plt.show()\n        IMAGE_DATA.append((crop_img, id_num))\n\nTRAINING_IMAGES = [(cv2.resize(crop, (TARGET_SIZE[1], TARGET_SIZE[0])), id_num) for crop, id_num in IMAGE_DATA]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T20:56:12.602965Z","iopub.execute_input":"2025-05-02T20:56:12.603723Z","iopub.status.idle":"2025-05-02T20:56:12.704704Z","shell.execute_reply.started":"2025-05-02T20:56:12.603699Z","shell.execute_reply":"2025-05-02T20:56:12.703831Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(TRAINING_IMAGES)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T20:56:14.868382Z","iopub.execute_input":"2025-05-02T20:56:14.868676Z","iopub.status.idle":"2025-05-02T20:56:14.874046Z","shell.execute_reply.started":"2025-05-02T20:56:14.868654Z","shell.execute_reply":"2025-05-02T20:56:14.873217Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Calculating Pitch","metadata":{}},{"cell_type":"code","source":"def calculate_pitch(cy, staff_center, unit_size):\n    pitch_step = round(-(cy - staff_center) / (unit_size / 2))\n    \n    pitch_names = {\n    -20: \"C\", -19: \"D\", -18: \"E\", -17: \"F\", -16: \"G\",\n    -15: \"A\", -14: \"B\", -13: \"C\", -12: \"D\", -11: \"E\",\n    -10: \"F\",  -9: \"G\",  -8: \"A\",  -7: \"B\",  -6: \"C\",\n     -5: \"D\",  -4: \"E\",  -3: \"F\",  -2: \"G\",  -1: \"A\",\n      0: \"B\",   1: \"C\",   2: \"D\",   3: \"E\",   4: \"F\",\n      5: \"G\",   6: \"A\",   7: \"B\",   8: \"C\",   9: \"D\",\n     10: \"E\",  11: \"F\",  12: \"G\",  13: \"A\",  14: \"B\",\n     15: \"C\",  16: \"D\",  17: \"E\",  18: \"F\",  19: \"G\",\n     20: \"A\"\n    }\n\n    pitch = pitch_names.get(pitch_step, \"Unknown\")\n    return pitch, pitch_step","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T20:56:16.620691Z","iopub.execute_input":"2025-05-02T20:56:16.621366Z","iopub.status.idle":"2025-05-02T20:56:16.627642Z","shell.execute_reply.started":"2025-05-02T20:56:16.621340Z","shell.execute_reply":"2025-05-02T20:56:16.626798Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def calculate_pitch_multi(y, staffs):\n    \"\"\"\n    Pick the correct staff for a note at vertical position y,\n    then compute its pitch & step using that staff's center & unit_size.\n    \"\"\"\n    # 1) find any staff whose bounds contain y\n    for st in staffs:\n        if st.upper_bound <= y <= st.lower_bound:\n            return calculate_pitch(y, st.center, st.unit_size)\n\n    # 2) if none matched perfectly, fall back to the *closest* staff center\n    closest = min(staffs, key=lambda s: abs(s.center - y))\n    return calculate_pitch(y, closest.center, closest.unit_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T20:56:18.143543Z","iopub.execute_input":"2025-05-02T20:56:18.144176Z","iopub.status.idle":"2025-05-02T20:56:18.149600Z","shell.execute_reply.started":"2025-05-02T20:56:18.144152Z","shell.execute_reply":"2025-05-02T20:56:18.148640Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Multiclass Classification","metadata":{}},{"cell_type":"markdown","source":"## Loading Dataset","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_pickle('/kaggle/input/multiclass-noteheads/df_trunc.pkl')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T20:56:19.521147Z","iopub.execute_input":"2025-05-02T20:56:19.521706Z","iopub.status.idle":"2025-05-02T20:56:19.530388Z","shell.execute_reply.started":"2025-05-02T20:56:19.521681Z","shell.execute_reply":"2025-05-02T20:56:19.529524Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mapping = {\n    \"noteheadBlack\": 1,\n    \"noteheadHalf\":  0,\n    \"noteheadWhole\": 0\n}\n\ndf['label_binary'] = df['label'].map(mapping)\nprint(df['label_binary'].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T20:56:20.632336Z","iopub.execute_input":"2025-05-02T20:56:20.633185Z","iopub.status.idle":"2025-05-02T20:56:20.640312Z","shell.execute_reply.started":"2025-05-02T20:56:20.633159Z","shell.execute_reply":"2025-05-02T20:56:20.639571Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Building CNN \n\nThis binary classifier will identify if notes are hollow or not","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import (Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization)\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nIMAGE_SHAPE = (39, 23, 1)\n\nbinary_model = Sequential([\n    # First Convolutional Block\n    Conv2D(32, (3,3), activation='relu', input_shape=IMAGE_SHAPE),\n    BatchNormalization(),\n    MaxPooling2D((2,2)),\n    \n    # Second Convolutional Block\n    Conv2D(64, (3,3), activation='relu'),\n    BatchNormalization(),\n    MaxPooling2D((2,2)),\n    \n    # Classifier\n    Flatten(),\n    Dense(128, activation='relu'),\n    Dropout(0.5),\n\n    # Single–unit sigmoid output for binary classification\n    Dense(1, activation='sigmoid')\n])\n\nbinary_model.summary()\n\nbinary_model.compile(\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T20:56:22.300412Z","iopub.execute_input":"2025-05-02T20:56:22.300951Z","iopub.status.idle":"2025-05-02T20:56:22.384279Z","shell.execute_reply.started":"2025-05-02T20:56:22.300924Z","shell.execute_reply":"2025-05-02T20:56:22.383513Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX = df[\"image\"].tolist()\nX = np.stack(X) # Stacking arrays into a tensor\n\nprint(\"Data shape\", X.shape)\n\ny = df[\"label_binary\"].values\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X,y,\n    test_size = 0.2, # 80/20 split\n    stratify=y, # Need equal parts of each class in each\n    random_state=42\n)\nprint(\"Train class distribution:\", np.bincount(y_train))\nprint(\"Test  class distributino:\", np.bincount(y_test))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T20:56:24.651537Z","iopub.execute_input":"2025-05-02T20:56:24.652205Z","iopub.status.idle":"2025-05-02T20:56:24.661652Z","shell.execute_reply.started":"2025-05-02T20:56:24.652171Z","shell.execute_reply":"2025-05-02T20:56:24.660820Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"result = binary_model.fit(\n    X_train, y_train,\n    validation_data=(X_test, y_test),\n    epochs=50,\n    batch_size=32\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T20:56:25.932073Z","iopub.execute_input":"2025-05-02T20:56:25.932366Z","iopub.status.idle":"2025-05-02T20:56:47.649917Z","shell.execute_reply.started":"2025-05-02T20:56:25.932347Z","shell.execute_reply":"2025-05-02T20:56:47.649214Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"probs = binary_model.predict(X_test).ravel() \npredicted = (probs > 0.5).astype(int) \n\nreverse_mapping = {1:\"Not Hollow\", 0:\"Hollow\"}\n\nn = 10\nindexes = np.random.choice(len(X_test), size=n, replace=False)\n\nfig, axes = plt.subplots(2, 5, figsize=(15,6))\naxes = axes.flatten()\n\nfor ax, i in zip(axes, indexes):\n    img  = X_test[i].squeeze()\n    true_lbl = y_test[i]\n    pred_lbl = predicted[i]  \n\n    ax.imshow(img, cmap='gray')\n    ax.set_title(\n        f\"True: {reverse_mapping[true_lbl]}\\n\"\n        f\"Pred: {reverse_mapping[pred_lbl]}\"\n    )\n    ax.axis('off')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T20:57:52.227945Z","iopub.execute_input":"2025-05-02T20:57:52.228330Z","iopub.status.idle":"2025-05-02T20:57:53.184430Z","shell.execute_reply.started":"2025-05-02T20:57:52.228306Z","shell.execute_reply":"2025-05-02T20:57:53.183507Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"binary_model.save(\"binary_model.h5\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T20:57:55.308449Z","iopub.execute_input":"2025-05-02T20:57:55.309226Z","iopub.status.idle":"2025-05-02T20:57:55.350401Z","shell.execute_reply.started":"2025-05-02T20:57:55.309193Z","shell.execute_reply":"2025-05-02T20:57:55.349341Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Predicting on images from filtering ","metadata":{}},{"cell_type":"code","source":"import cv2\n\ndf_images = pd.DataFrame(TRAINING_IMAGES, columns=['image', 'id'])\n\ndef binarize_and_invert(img, thresh=127):\n    _, binary = cv2.threshold(img, thresh, 255, cv2.THRESH_BINARY)\n    inverted = 255 - binary\n    return inverted\n\ndf_images['binarized'] = df_images['image'].apply(binarize_and_invert)\n\n# Quick sanity check on shapes and dtypes\nprint(df_images[['id', 'binarized']].head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T20:57:56.701911Z","iopub.execute_input":"2025-05-02T20:57:56.702895Z","iopub.status.idle":"2025-05-02T20:57:56.766302Z","shell.execute_reply.started":"2025-05-02T20:57:56.702863Z","shell.execute_reply":"2025-05-02T20:57:56.765508Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X = df_images[\"binarized\"].tolist()\nX = np.stack(X) \nX = X[..., np.newaxis].astype(\"float32\") / 255.0\n\nprobs = binary_model.predict(X)\npreds = (probs > 0.5).astype(int)\n\npreds_flat = preds.ravel() \nid_to_pred = dict(zip(df_images[\"id\"], preds_flat))\nid_to_pred = {int(k): int(v) for k, v in id_to_pred.items()}\n\nprint(id_to_pred)\n\ndf_images['pred'] = df_images['id'].map(id_to_pred)\ndf_zero = df_images[df_images['pred'] == 0].copy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T20:58:00.875060Z","iopub.execute_input":"2025-05-02T20:58:00.875637Z","iopub.status.idle":"2025-05-02T20:58:01.918417Z","shell.execute_reply.started":"2025-05-02T20:58:00.875612Z","shell.execute_reply":"2025-05-02T20:58:01.917450Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\nX = np.stack(df_images[\"binarized\"].tolist(), axis=0)\nX = X[..., np.newaxis].astype(\"float32\") / 255.0\n\nprobs = binary_model.predict(X).ravel()\npreds = (probs > 0.5).astype(int)\n\nreverse_mapping = {0:\"Hollow\", 1:\"Not Hollow\"}\n\nn = 10\nindexes = np.random.choice(len(X), size=n, replace=False)\n\nfig, axes = plt.subplots(2, 5, figsize=(15,6))\nfor ax, i in zip(axes.flatten(), indexes):\n    img = X[i].squeeze()\n    pred_lbl = preds[i]\n    label_txt = reverse_mapping[pred_lbl]\n\n    ax.imshow(img, cmap='gray')\n    ax.set_title(f\"{label_txt}\\n\")\n    ax.axis('off')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T20:58:06.321261Z","iopub.execute_input":"2025-05-02T20:58:06.322122Z","iopub.status.idle":"2025-05-02T20:58:08.358514Z","shell.execute_reply.started":"2025-05-02T20:58:06.322097Z","shell.execute_reply":"2025-05-02T20:58:08.357706Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Whole Note vs Half Note CNN","metadata":{}},{"cell_type":"code","source":"import pickle\n\nwith open('/kaggle/input/multiclass-noteheads/df_full', 'rb') as f:\n    df_whole_half = pickle.load(f)\n\nprint(df_whole_half.head)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T20:58:10.185392Z","iopub.execute_input":"2025-05-02T20:58:10.185692Z","iopub.status.idle":"2025-05-02T20:58:10.244438Z","shell.execute_reply.started":"2025-05-02T20:58:10.185671Z","shell.execute_reply":"2025-05-02T20:58:10.243627Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"whole_half_df = df_whole_half[df_whole_half['label'].isin(['noteheadWhole','noteheadHalf'])].copy()\n\nwhole_half_df['y'] = whole_half_df['label'].map({'noteheadHalf': 0, 'noteheadWhole':1})\n\nheight, width = 23, 39\nX = np.stack(whole_half_df['image'].values)\nX = X.reshape(-1, height, width, 1).astype('float32')\ny = whole_half_df['y'].values\n\nX_train, X_val, y_train, y_val = train_test_split(\n    X, y, test_size=0.2, stratify=y, random_state=42\n)\n\nprint(whole_half_df['y'].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T20:58:12.087492Z","iopub.execute_input":"2025-05-02T20:58:12.087817Z","iopub.status.idle":"2025-05-02T20:58:12.102069Z","shell.execute_reply.started":"2025-05-02T20:58:12.087795Z","shell.execute_reply":"2025-05-02T20:58:12.101126Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\ndef show_equal_by_class(X, y, label_map):\n\n    np.random.seed(42)\n    \n    classes = np.unique(y)\n    # collect indices for each class\n    idcs = []\n    for cls in classes:\n        cls_idx = np.where(y == cls)[0]\n        chosen = np.random.choice(cls_idx, 5, replace=False)\n        idcs.append(chosen)\n    \n    # flatten and prepare grid\n    idcs = np.concatenate(idcs)\n    rows = len(classes)\n    cols = n_per_class\n    fig, axes = plt.subplots(rows, cols, figsize=(2*cols, 2*rows))\n    \n    for row, cls in enumerate(classes):\n        for col in range(n_per_class):\n            ax = axes[row, col] if rows>1 else axes[col]\n            i = idcs[row*n_per_class + col]\n            img = X[i].squeeze()\n            ax.imshow(img, cmap='gray')\n            ax.set_title(label_map[cls], fontsize=8)\n            ax.axis('off')\n    \n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T20:58:13.273054Z","iopub.execute_input":"2025-05-02T20:58:13.273705Z","iopub.status.idle":"2025-05-02T20:58:13.281059Z","shell.execute_reply.started":"2025-05-02T20:58:13.273682Z","shell.execute_reply":"2025-05-02T20:58:13.280109Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.layers import (Input, Conv2D, BatchNormalization, Activation, MaxPooling2D, Dropout, GlobalAveragePooling2D, Dense)\nfrom tensorflow.keras.models import Model \nfrom tensorflow.keras import regularizers\n\nIMAGE_SHAPE = (height, width, 1)\nWEIGHT_DECAY = 1e-4\nreg = regularizers.l2(WEIGHT_DECAY)\n\nhollow_model = Sequential([\n    Conv2D(32, (3,3), padding='same', activation='relu', kernel_regularizer=reg, input_shape=IMAGE_SHAPE), \n    BatchNormalization(),\n    MaxPooling2D(2),\n    Dropout(0.25), \n\n    Conv2D(64, (3,3), padding='same', activation='relu', kernel_regularizer=reg),\n    BatchNormalization(),\n    MaxPooling2D(2),\n    Dropout(0.35),\n\n    Conv2D(128, (3,3), padding='same', activation='relu',\n           kernel_regularizer=reg),\n    BatchNormalization(),\n    MaxPooling2D(2),\n    Dropout(0.45),\n\n    GlobalAveragePooling2D(),\n    Dense(64, activation='relu', kernel_regularizer=reg),\n    BatchNormalization(),\n    Dropout(0.5),\n\n    Dense(1, activation='sigmoid', kernel_regularizer=reg)\n])\n\nhollow_model.compile(\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\n\nhollow_model.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T20:58:15.220840Z","iopub.execute_input":"2025-05-02T20:58:15.221154Z","iopub.status.idle":"2025-05-02T20:58:15.342746Z","shell.execute_reply.started":"2025-05-02T20:58:15.221131Z","shell.execute_reply":"2025-05-02T20:58:15.342011Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n\ndatagen = ImageDataGenerator(\n    rotation_range=15,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    zoom_range=0.15,\n    fill_mode='nearest'\n)\ndatagen.fit(X_train)\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n\nhistory = hollow_model.fit(\n    datagen.flow(X_train, y_train, batch_size=32),\n    validation_data=(X_val, y_val),\n    epochs=25,\n)\n\nplt.plot(history.history['loss'], label='train')\nplt.plot(history.history['val_loss'], label='val')\nplt.legend(); plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T20:58:19.821466Z","iopub.execute_input":"2025-05-02T20:58:19.821816Z","iopub.status.idle":"2025-05-02T20:58:50.211405Z","shell.execute_reply.started":"2025-05-02T20:58:19.821753Z","shell.execute_reply":"2025-05-02T20:58:50.210443Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"hollow_model.save(\"hollow_model.h5\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T20:59:02.585912Z","iopub.execute_input":"2025-05-02T20:59:02.586245Z","iopub.status.idle":"2025-05-02T20:59:02.638611Z","shell.execute_reply.started":"2025-05-02T20:59:02.586221Z","shell.execute_reply":"2025-05-02T20:59:02.637717Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"probs = hollow_model.predict(X_val).ravel()\npredicted = (probs > 0.5).astype(int)\n\nreverse_mapping = {0: \"half note\", 1: \"whole note\"}\n\nn=10\nindexes = np.random.choice(len(X_val), size=n, replace=False)\n\nfig, axes = plt.subplots(2, 5, figsize=(15, 6))\naxes = axes.flatten()\n\nfor ax, idx in zip(axes, indexes):\n    img = X_val[idx].squeeze()    # (39,23) image\n    true_lbl = y_val[idx]         # 0 or 1\n    pred_lbl = predicted[idx]      # 0 or 1\n\n    ax.imshow(img, cmap='gray')\n    ax.set_title(\n        f\"True: {reverse_mapping[true_lbl]}\\n\"\n        f\"Pred: {reverse_mapping[pred_lbl]}\"\n    )\n    ax.axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T20:59:04.533451Z","iopub.execute_input":"2025-05-02T20:59:04.534278Z","iopub.status.idle":"2025-05-02T20:59:05.632604Z","shell.execute_reply.started":"2025-05-02T20:59:04.534251Z","shell.execute_reply":"2025-05-02T20:59:05.631807Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_val, predicted)\nprint(\"             Pred half   Pred whole\")\nprint(f\"True half    {cm[0,0]:4d}         {cm[0,1]:4d}\")\nprint(f\"True whole   {cm[1,0]:4d}         {cm[1,1]:4d}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T20:59:06.974200Z","iopub.execute_input":"2025-05-02T20:59:06.974742Z","iopub.status.idle":"2025-05-02T20:59:06.982236Z","shell.execute_reply.started":"2025-05-02T20:59:06.974717Z","shell.execute_reply":"2025-05-02T20:59:06.981386Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\nX = np.stack(df_zero[\"binarized\"].tolist(), axis=0)\nX = X[..., np.newaxis].astype(\"float32\") / 255.0\n\nprobs = hollow_model.predict(X).ravel()\npredicted = (probs > 0.5).astype(int)\n\nreverse_mapping = {0: \"half note\", 1: \"whole note\"}\n\nn=10\nindexes = np.random.choice(len(X), size=n, replace=False)\n\nfig, axes = plt.subplots(2, 5, figsize=(15, 6))\naxes = axes.flatten()\n\nfor ax, idx in zip(axes, indexes):\n    img = X[idx].squeeze()    # (39,23) image\n    pred_lbl = predicted[idx] # 0 or 1\n    \n    ax.imshow(img, cmap='gray')\n    ax.set_title(\n        f\"Pred: {reverse_mapping[pred_lbl]}\"\n    )\n    ax.axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T20:59:07.966121Z","iopub.execute_input":"2025-05-02T20:59:07.966776Z","iopub.status.idle":"2025-05-02T20:59:09.472736Z","shell.execute_reply.started":"2025-05-02T20:59:07.966731Z","shell.execute_reply":"2025-05-02T20:59:09.471751Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Final Image","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom PIL import Image\n\nfor idx in range(10):\n    img, notes, _ = ACCEPTED_NOTES[idx]\n    img_pil = Image.fromarray(img)\n    staff_objects = extract_staffs_from_image(img_pil)\n\n    fig, ax = plt.subplots(figsize=(14, 8))\n    ax.imshow(img_pil, cmap='gray')\n    ax.set_title(f\"Image {idx}: Note Positions with Pitches\")\n    for (((cx, cy), (MA, ma), ang), num_id) in notes:\n        pred_lbl = reverse_mapping[id_to_pred[num_id]]\n        pitch, step = calculate_pitch_multi(cy, staff_objects)\n    \n        circ = plt.Circle((cx, cy), radius=5, color='red', fill=True)\n        ax.add_patch(circ)\n        ax.text(cx + 10, cy + 20, f\"{pitch}\", color='blue', fontsize=9, verticalalignment='center')\n        \n    ax.axis('off')\n    plt.show()\n\n    fig, ax = plt.subplots(figsize=(14, 8))\n    ax.imshow(img_pil, cmap='gray')\n    ax.set_title(f\"Image {idx}: Note Positions with Classification\")\n    for (((cx, cy), (MA, ma), ang), num_id) in notes:\n        ax.text(cx + 10, cy + 20, f\"{id_to_pred[num_id]}\", color='blue', fontsize=9, verticalalignment='center')\n    ax.axis('off')\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T20:59:09.914185Z","iopub.execute_input":"2025-05-02T20:59:09.914478Z","iopub.status.idle":"2025-05-02T20:59:22.059297Z","shell.execute_reply.started":"2025-05-02T20:59:09.914457Z","shell.execute_reply":"2025-05-02T20:59:22.058442Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Accuracy Check","metadata":{}},{"cell_type":"code","source":"id_to_info = {\n    id_num: {\n        \"cx\":           float(cx),\n        \"cy\":           float(cy),\n        \"pred\":         int(id_to_pred[id_num]),   # or id_to_label[id_num]\n        \"source_image\": image_name\n    }\n    for gray, accepted_list, image_name in ACCEPTED_NOTES\n    for ((cx, cy), (_, _), _), id_num in accepted_list\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T22:45:39.784324Z","iopub.execute_input":"2025-05-02T22:45:39.784698Z","iopub.status.idle":"2025-05-02T22:45:39.794241Z","shell.execute_reply.started":"2025-05-02T22:45:39.784673Z","shell.execute_reply":"2025-05-02T22:45:39.793370Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom PIL import Image\nimport os\n\ndef show_note_by_id(id_num,\n                    id_to_info,\n                    accepted_notes,\n                    doremi_dir=\"/kaggle/input/doremi-raw-dataset/DoReMi_v1/Images\"):\n    # 1) Lookup the meta‐info\n    info = id_to_info.get(id_num)\n    if info is None:\n        print(f\"ID {id_num} not in id_to_info.\")\n        return\n\n    src_name = info[\"source_image\"]\n    cx, cy   = info[\"cx\"], info[\"cy\"]\n\n    # 2) Try the cropped staff image first\n    for img_crop, notes_list, img_name in accepted_notes:\n        if img_name != src_name:\n            continue\n        for ((x, y), _, _), nid in notes_list:\n            if nid == id_num:\n                # Plot cropped‐staff\n                fig, ax = plt.subplots(figsize=(6, 4))\n                ax.imshow(img_crop, cmap=\"gray\")\n                ax.scatter([x], [y], c=\"red\", s=120, marker=\"o\")\n                ax.set_title(f\"Note ID {id_num} in {img_name}\")\n                ax.axis(\"off\")\n                plt.show()\n                plt.close(fig)\n                return\n\n    full_path = os.path.join(doremi_dir, src_name)\n    full = Image.open(full_path).convert(\"L\")\n    fig, ax = plt.subplots(figsize=(6, 8))\n    ax.imshow(full, cmap=\"gray\")\n    ax.scatter([cx], [cy], c=\"red\", s=120, marker=\"o\")\n    ax.set_title(f\"(fallback) Note ID {id_num} on full image {src_name}\")\n    ax.axis(\"off\")\n    plt.show()\n    plt.close(fig)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T22:45:40.657503Z","iopub.execute_input":"2025-05-02T22:45:40.658304Z","iopub.status.idle":"2025-05-02T22:45:40.667247Z","shell.execute_reply.started":"2025-05-02T22:45:40.658274Z","shell.execute_reply":"2025-05-02T22:45:40.666354Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport xml.etree.ElementTree as ET\n\nomr_dir = \"/kaggle/input/doremi-raw-dataset/DoReMi_v1/Parsed_by_page_omr_xml\"\n\nfirst_ids = list(id_to_info.keys())\n\nid_to_xml_node = {}\nfor id_num in first_ids:\n    info = id_to_info[id_num]\n    img_name = info[\"source_image\"]\n    \n    omr_file = image_to_omr.get(img_name)\n    if omr_file is None:\n        print(f\"No OMR XML for image {img_name}\")\n        continue\n    \n    path = os.path.join(omr_dir, omr_file)\n    tree = ET.parse(path)\n    root = tree.getroot()\n    \n    candidates = []\n    for node in root.findall(\".//Node\"):\n        cn = node.find(\"ClassName\")\n        if cn is None or cn.text not in {\"noteheadBlack\",\"noteheadHalf\",\"noteheadWhole\"}:\n            continue\n        t = int(node.find(\"Top\").text)\n        l = int(node.find(\"Left\").text)\n        w = int(node.find(\"Width\").text)\n        h = int(node.find(\"Height\").text)\n        candidates.append((node, l, t, l+w, t+h))\n    \n    cx = info[\"cx\"]\n    cy = info[\"cy\"] + max_upper\n    \n\n    match = None\n    for node, x0, y0, x1, y1 in candidates:\n        if x0 <= cx <= x1 and y0 <= cy <= y1:\n            match = node\n            break\n    \n    if match is None and candidates:\n        dists = []\n        for node, x0, y0, x1, y1 in candidates:\n            mx, my = (x0+x1)/2, (y0+y1)/2\n            d = ((mx-cx)**2 + (my-cy)**2)**0.5\n            dists.append((d, node))\n        match = min(dists, key=lambda x: x[0])[1]\n    \n    if match is not None:\n        id_to_xml_node[id_num] = match\n    else:\n        print(f\"ID {id_num} at ({cx:.1f},{cy:.1f}) in {img_name} had no OMR XML nodes\")\n\nfor id_num in first_ids:\n    node = id_to_xml_node.get(id_num)\n    if not node:\n        continue\n    info = id_to_info[id_num]\n    info.update({\n      \"xml_class\":  node.find(\"ClassName\").text,\n      \"xml_top\":    int(node.find(\"Top\").text),\n      \"xml_left\":   int(node.find(\"Left\").text),\n      \"xml_width\":  int(node.find(\"Width\").text),\n      \"xml_height\": int(node.find(\"Height\").text)\n    })","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T22:45:42.324835Z","iopub.execute_input":"2025-05-02T22:45:42.325178Z","iopub.status.idle":"2025-05-02T22:46:03.116626Z","shell.execute_reply.started":"2025-05-02T22:45:42.325154Z","shell.execute_reply":"2025-05-02T22:46:03.115811Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\npage_names = list(IMAGE_TO_NOTES.keys())\n\ngt_counts        = []\ndet_counts       = []\nclass_accuracies = []\n\nfor page in page_names:\n    gt = len(IMAGE_TO_NOTES[page])\n    det_ids = [nid for nid, info in id_to_info.items() if info[\"source_image\"] == page]\n    det = len(det_ids)\n\n    correct = 0\n    for nid in det_ids:\n        info = id_to_info[nid]\n        gt_label = 1 if info[\"xml_class\"] == \"noteheadBlack\" else 0\n        correct += (info[\"pred\"] == gt_label)\n    acc = correct / det if det>0 else 0.0\n\n    gt_counts.append(gt)\n    det_counts.append(det)\n    class_accuracies.append(acc)\n\n# Build abbreviated labels: \"Composer-Page\"\nabbr_labels = [\n    f\"{name.split(' - ')[0]}-{name.rsplit('-',1)[-1].split('.')[0]}\"\n    for name in page_names\n]\n\nind   = np.arange(len(page_names))\nwidth = 0.35\n\nfig, ax = plt.subplots(figsize=(14,5))\nax.bar(ind,        gt_counts,  width, label=\"Ground-truth (XML)\")\nax.bar(ind+width,  det_counts, width, label=\"Detected\")\nax.set_xticks(ind + width/2)\nax.set_xticklabels(\n    abbr_labels,\n    rotation=90,\n    ha=\"center\",\n    fontsize=6     \n)\nax.set_ylabel(\"Number of notes\")\nax.set_title(\"Per-page: GT vs. Detected noteheads\")\nax.legend()\nplt.tight_layout()\nplt.show()\n\nfig, ax = plt.subplots(figsize=(14,3))\nax.bar(ind, class_accuracies, color=\"tab:green\", width=0.6)\nax.set_xticks(ind)\nax.set_xticklabels(\n    abbr_labels,\n    rotation=90,\n    ha=\"center\",\n    fontsize=6      \n)\nax.set_ylim(0.9, 1.0)\nax.set_ylabel(\"Accuracy\")\nax.set_title(\"Per-page notehead classification accuracy\")\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T22:46:03.117912Z","iopub.execute_input":"2025-05-02T22:46:03.118160Z","iopub.status.idle":"2025-05-02T22:46:04.786735Z","shell.execute_reply.started":"2025-05-02T22:46:03.118135Z","shell.execute_reply":"2025-05-02T22:46:04.785832Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Appendix\n\nReference Github: https://github.com/BreezeWhite/oemer\n\nImporting sample image from the [DoReMi](https://github.com/steinbergmedia/DoReMi/?tab=readme-ov-file#OMR-metadata) dataset.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}